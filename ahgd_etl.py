# -*- coding: utf-8 -*-
"""AHGD_ETL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kEFmdjbNCBWmQLp7-x1lcyognRvMI7VU
"""

# ==============================================================================
# Cell 0: Notebook Title and Introduction
# ==============================================================================

# @title # AHGD ETL Pipeline (Geo + Population) - Colab Version
# @markdown This notebook performs the Extract, Transform, Load (ETL) process to create:
# @markdown 1.  `geo_dimension.parquet`: Geographic boundaries and attributes from ABS ASGS.
# @markdown 2.  `population_dimension.parquet`: Population counts (Total, M/F, Indigenous) from ABS Census G01.
# @markdown
# @markdown **Instructions:**
# @markdown 1.  **Run Cell 1** to mount your Google Drive. Follow the authentication prompts.
# @markdown 2.  **!! IMPORTANT !! Modify `drive_base_path_str` in Cell 2** to your desired project location on Google Drive.
# @markdown 3.  **Run Cell 2** to define and verify your project path.
# @markdown 4.  **Run Cell 3** to install required Python packages.
# @markdown 5.  **Run Cell 4** to define configuration and utility functions.
# @markdown 6.  **Run Cell 5** to define the core geography and census processing logic.
# @markdown 7.  **Run Cell 6** to define the main pipeline orchestration function.
# @markdown 8.  **Run Cell 7** to execute the entire ETL pipeline. Monitor the output and logs.
# @markdown 9.  Check the specified output directory on your Google Drive for the Parquet files upon successful completion.

# ==============================================================================
# Cell 1: Mount Google Drive
# ==============================================================================
# @markdown Mounts your Google Drive to `/content/drive`. You will need to authenticate.
from google.colab import drive
import os
from pathlib import Path # Import Path early

drive_mount_point = '/content/drive'
my_drive_path = Path(drive_mount_point) / 'MyDrive/' # Standard path to your Drive root

print(f"Attempting to mount Google Drive at {drive_mount_point}...")
try:
    # force_remount=True can help avoid issues with stale mounts if you re-run the notebook
    drive.mount(drive_mount_point, force_remount=True)
    print("Google Drive mounted successfully.")
    # Check if MyDrive is accessible
    if not my_drive_path.exists():
         print(f"Warning: Standard path {my_drive_path} not found immediately after mount.")
         print("This might be okay, but check the next cell carefully.")
    else:
        print(f"Confirmed access to: {my_drive_path}")

except Exception as e:
    print(f"ERROR mounting Google Drive: {e}")
    print("Please ensure you followed the authentication prompts correctly.")
    # Stop execution if Drive mounting fails, as everything depends on it
    raise SystemExit("Google Drive mount failed. Cannot continue.")

print("\n--> Proceed to Cell 2 to configure your project path.")

# ==============================================================================
# Cell 2: Define and Verify Project Path on Google Drive
# ==============================================================================
# @markdown **Action Required:** Modify `drive_base_path_str` below to the *exact*
# @markdown path within your Google Drive where you want project data (raw, temp, output, logs)
# @markdown to be stored. It will be created if it doesn't exist.
# @markdown Example: `/content/drive/MyDrive/Colab_Projects/AHGD_ETL`

# --- IMPORTANT: Define Your Project Path on Google Drive ---
drive_base_path_str = '/content/drive/MyDrive/Colab_Notebooks/AHGD_Project' # <<<-------- CHANGE THIS AS NEEDED
# -----------------------------------------------------------

DRIVE_PROJECT_PATH = Path(drive_base_path_str)
print(f"Target Project Path set to: {DRIVE_PROJECT_PATH}")

print("\nVerifying project path and creating if necessary...")
try:
    # Create the directory and intermediate parents if they don't exist
    DRIVE_PROJECT_PATH.mkdir(parents=True, exist_ok=True)
    print(f"Project base directory ensured at: {DRIVE_PROJECT_PATH}")

    # --- Crucial Check: Writability ---
    # Google Drive File System (DriveFS) can sometimes mount but be unwritable initially.
    test_file_path = DRIVE_PROJECT_PATH / ".writable_test"
    try:
        with open(test_file_path, 'w') as f:
            f.write('test')
        test_file_path.unlink() # Clean up test file
        print(f"Successfully wrote and deleted test file in {DRIVE_PROJECT_PATH}. Path is writable.")
        # Define BASE_DIR globally for other cells *after* verification
        global BASE_DIR
        BASE_DIR = DRIVE_PROJECT_PATH
        print("--> Project path verified and seems ready. Proceed to Cell 3.")
    except Exception as write_error:
        print(f"ERROR: Failed write test in {DRIVE_PROJECT_PATH}: {write_error}")
        print("The project directory might exist but is not writable by Colab.")
        print("Common Causes:")
        print("  - Google Drive sync issues (wait a minute and retry Cell 1 & 2).")
        print("  - Incorrect path specified above.")
        print("  - Permissions issues within Google Drive.")
        raise SystemExit(f"Cannot write to project directory: {DRIVE_PROJECT_PATH}. Stopping.")

except Exception as e:
     print(f"ERROR: Could not create or access base directory {DRIVE_PROJECT_PATH}: {e}")
     raise SystemExit(f"Failed to ensure project directory: {DRIVE_PROJECT_PATH}. Stopping.")

# ==============================================================================
# Cell 3: Install Required Packages
# ==============================================================================
# @markdown Installs necessary Python libraries using pip. Output is suppressed for brevity ('--quiet').
# @markdown Remove `--quiet` if you need to debug installation issues.

print("Installing required packages (pandas, geopandas, polars, etc.)...")
!pip install pandas geopandas polars requests tqdm pyarrow shapely openpyxl --quiet
print("Package installation command executed.")
print("Verification: Attempting imports...")
try:
    import pandas as pd
    import geopandas as gpd
    import polars as pl
    import requests
    import tqdm
    import pyarrow
    import shapely
    import openpyxl
    print("Core libraries imported successfully.")
    print("--> Proceed to Cell 4.")
except ImportError as e:
    print(f"\nERROR: Failed to import a library after installation: {e}")
    print("This likely means installation failed. Please check the output above")
    print("(remove '--quiet' from the !pip install command and re-run this cell to see details).")
    raise SystemExit("Package installation/import failed.")

# ================================================
# Cell 4: Configuration & Utility Functions
# ================================================
# @markdown Defines project configuration (paths, URLs, schemas) and reusable helper functions (logging, download, extract, data cleaning).

import logging
import zipfile
import requests
from pathlib import Path
import tempfile
import shutil
import time
import re
import json
from typing import List, Dict, Optional, Union, Any # Corrected imports
import os
import sys

# Third-party libraries (re-import inside functions if needed, but define top-level for clarity)
import pandas as pd
import geopandas as gpd
import polars as pl
import pyarrow
import pyarrow.parquet as pq
from shapely.geometry import mapping # Keep for potential use
from shapely.validation import make_valid # More robust repair method
from tqdm.notebook import tqdm # Use notebook-friendly tqdm
import polars.exceptions

# --- 1. Configuration Settings ---
# Uses BASE_DIR defined and verified in Cell 2

# Check if BASE_DIR is defined (safety check)
if 'BASE_DIR' not in globals() or not isinstance(BASE_DIR, Path):
    raise NameError("BASE_DIR is not defined globally. Please run Cell 2 first.")

# Ensure BASE_DIR exists and is writable (redundant if Cell 2 check passed, but safe)
try:
    if not BASE_DIR.exists():
        BASE_DIR.mkdir(parents=True, exist_ok=True)
    # Perform a simple write test again just in case
    test_file = BASE_DIR / ".write_test"
    test_file.write_text("test")
    test_file.unlink()
except Exception as e:
    raise SystemExit(f"Error: BASE_DIR '{BASE_DIR}' is not accessible or writable: {e}")

print(f"Using BASE_DIR: {BASE_DIR}")

# Define derived paths
DATA_DIR = BASE_DIR / 'data'
RAW_DATA_DIR = DATA_DIR / 'raw'
OUTPUT_DIR = BASE_DIR / 'output' # Output Parquet files go here
TEMP_DIR = RAW_DATA_DIR / 'temp'
LOG_DIR = BASE_DIR / 'logs'
LOG_FILE = LOG_DIR / 'ahgd_colab_run.log'

# Specific data directories (within Google Drive via BASE_DIR)
GEOGRAPHIC_DIR = RAW_DATA_DIR / 'geographic' # Shapefiles might end up here if not zipped
CENSUS_DIR = RAW_DATA_DIR / 'census'
TEMP_ZIP_DIR = TEMP_DIR / "zips" # Downloaded zips stored here
TEMP_EXTRACT_DIR = TEMP_DIR / "extract" # Extracted contents go here

# --- Data source URLs ---
# Use the standard prefix (LEVEL_YEAR_REGION_DATUM) as the key.
# The actual zip filename might vary slightly in the URL, but the key should be consistent.
# ***** VERIFY THESE URLS ARE CORRECT ON THE ABS WEBSITE *****
DATA_URLS = {
    # ASGS Main Structures (Using GDA2020 Shapefiles)
    # Key format: LEVEL_YEAR_REGION_DATUM
    'SA1_2021_AUST_GDA2020': 'https://www.abs.gov.au/statistics/standards/australian-statistical-geography-standard-asgs-edition-3/jul2021-jun2026/access-and-downloads/digital-boundary-files/SA1_2021_AUST_GDA2020_SHP.zip',
    'SA2_2021_AUST_GDA2020': 'https://www.abs.gov.au/statistics/standards/australian-statistical-geography-standard-asgs-edition-3/jul2021-jun2026/access-and-downloads/digital-boundary-files/SA2_2021_AUST_GDA2020_SHP.zip',
    'SA3_2021_AUST_GDA2020': 'https://www.abs.gov.au/statistics/standards/australian-statistical-geography-standard-asgs-edition-3/jul2021-jun2026/access-and-downloads/digital-boundary-files/SA3_2021_AUST_GDA2020_SHP.zip',
    'SA4_2021_AUST_GDA2020': 'https://www.abs.gov.au/statistics/standards/australian-statistical-geography-standard-asgs-edition-3/jul2021-jun2026/access-and-downloads/digital-boundary-files/SA4_2021_AUST_GDA2020_SHP.zip',
    'STE_2021_AUST_GDA2020': 'https://www.abs.gov.au/statistics/standards/australian-statistical-geography-standard-asgs-edition-3/jul2021-jun2026/access-and-downloads/digital-boundary-files/STE_2021_AUST_GDA2020_SHP.zip',
    # Non-ASGS Structures (Example: Postal Areas)
    'POA_2021_AUST_GDA2020': 'https://www.abs.gov.au/statistics/standards/australian-statistical-geography-standard-asgs-edition-3/jul2021-jun2026/access-and-downloads/digital-boundary-files/POA_2021_AUST_GDA2020_SHP.zip',

    # Census Data Packs (GCP)
    # Use a descriptive key, doesn't have to match the filename exactly here.
    # ***** REPLACE WITH ACTUAL URL FOR THE DATAPACK(S) NEEDED *****
    'CENSUS_GCP_AUS_2021': 'https://www.abs.gov.au/census/find-census-data/datapacks/download/2021_GCP_ALL_for_AUS.zip', # Example URL - Check ABS
}


# Geographic levels to process (Map internal names to the standard prefix from DATA_URLS keys)
GEO_LEVELS_SHP_PROCESS = {
    # Internal Name : Standard Prefix Key from DATA_URLS
    'SA1': 'SA1_2021_AUST_GDA2020',
    'SA2': 'SA2_2021_AUST_GDA2020',
    'SA3': 'SA3_2021_AUST_GDA2020',
    'SA4': 'SA4_2021_AUST_GDA2020',
    'STATE': 'STE_2021_AUST_GDA2020', # Map STATE internal name to STE prefix
    # 'POA': 'POA_2021_AUST_GDA2020' # Uncomment if processing POAs
}

# Geographic levels relevant for Census data processing
GEO_LEVELS_CENSUS_PROCESS = ['SA1', 'SA2'] # Which levels G01 data relates to

# Generate required ZIP URLs based on GEO_LEVELS_SHP_PROCESS
# Creates a dictionary {<zip_filename>: <url>}
REQUIRED_GEO_ZIPS = {}
for level, prefix in GEO_LEVELS_SHP_PROCESS.items():
    if prefix in DATA_URLS:
        zip_filename = f"{prefix}_SHP.zip" # Construct the zip filename from the prefix
        REQUIRED_GEO_ZIPS[zip_filename] = DATA_URLS[prefix] # Map filename to URL
    else:
        print(f"Warning: Prefix '{prefix}' for level '{level}' not found in DATA_URLS keys. Cannot download.")


# Census tables to process (using regex patterns for files inside zips)
CENSUS_TABLE_PATTERNS = {
    # Regex to find G01 CSV files for relevant geographies (SA1, SA2 etc.) within the Census zips
    "G01": r"2021\s*Census_G01[_\s].*?(" + "|".join(GEO_LEVELS_CENSUS_PROCESS) + r")\.csv$"
}

# REQUIRED_CENSUS_ZIPS - Define which zips contain the required Census tables
# Adjust this based on how ABS structures the DataPacks
# Uses the key defined in DATA_URLS ('CENSUS_GCP_AUS_2021')
REQUIRED_CENSUS_ZIPS = {}
census_key = 'CENSUS_GCP_AUS_2021' # Key used in DATA_URLS
if census_key in DATA_URLS:
     # Extract the actual filename from the URL to use as the key for download_data
     census_zip_filename = Path(DATA_URLS[census_key]).name
     REQUIRED_CENSUS_ZIPS[census_zip_filename] = DATA_URLS[census_key]
else:
     print(f"Warning: Census DataPack key '{census_key}' not found in DATA_URLS. Census data cannot be downloaded.")


# Logging Configuration
LOG_LEVEL = 'INFO' # Set to 'DEBUG' for more details, 'WARNING' for less
LOG_FORMAT = '%(asctime)s - %(name)s - %(levelname)s - %(message)s' # UK spelling: Initialised

# Define output schemas using Polars data types
GEO_DIMENSION_SCHEMA = {
    "geo_key": pl.Utf8,          # Primary key (e.g., "SA1_71234567890")
    "code": pl.Utf8,             # Geographic area code (e.g., "71234567890")
    "type": pl.Categorical,      # Geographic level (SA1, SA2, etc.)
    "name": pl.Utf8,             # Geographic area name
    "area_sqkm": pl.Float64,     # Area in square kilometres
    "geometry": pl.Utf8,         # WKT representation of the geometry
    "year": pl.Int32             # Reference year (e.g., 2021)
}

POPULATION_DIMENSION_SCHEMA = {
    "pop_key": pl.Utf8,              # Primary key (e.g., "POP_SA1_71234567890_2021")
    "geo_key": pl.Utf8,              # Foreign key to geo_dimension
    "year": pl.Int32,                # Reference year (e.g., 2021)
    "total_population": pl.Int64,    # Total population count
    "male_population": pl.Int64,     # Male population count
    "female_population": pl.Int64,   # Female population count
    "indigenous_population": pl.Int64 # Indigenous population count (Persons)
}


# --- 2. Logging Setup ---
# (Ensure logger is initialised before use in helper functions)
logger = None # Placeholder

def setup_logging():
    """Sets up logging configuration to file and console."""
    global logger
    if logger is not None:
        if logger.hasHandlers():
            return logger

    try:
        LOG_DIR.mkdir(parents=True, exist_ok=True)
        if not os.access(LOG_DIR, os.W_OK):
            raise OSError(f"Log directory '{LOG_DIR}' is not writable.")
        log_file_path = LOG_DIR / 'ahgd_colab_run.log'

    except Exception as e:
        print(f"Warning: Could not ensure log directory '{LOG_DIR}'. Logging to console only. Error: {e}", file=sys.stderr)
        log_file_path = None

    log_level_int = getattr(logging, LOG_LEVEL.upper(), logging.INFO)
    root_logger = logging.getLogger()
    root_logger.setLevel(log_level_int)

    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)
        handler.close()

    formatter = logging.Formatter(LOG_FORMAT)
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(log_level_int)
    console_handler.setFormatter(formatter)
    root_logger.addHandler(console_handler)

    if log_file_path:
        try:
            file_handler = logging.FileHandler(log_file_path, mode='a', encoding='utf-8')
            file_handler.setLevel(logging.DEBUG) # Log DEBUG level and above to file
            file_handler.setFormatter(formatter)
            root_logger.addHandler(file_handler)
        except Exception as e:
            print(f"Warning: Failed to create file handler for '{log_file_path}'. Logging to console only. Error: {e}", file=sys.stderr)

    for lib in ['fiona', 'shapely', 'geopandas', 'urllib3']:
        logging.getLogger(lib).setLevel(logging.WARNING)

    logger = logging.getLogger("ahgd_etl_colab")
    logger.info(f"Logging initialised at level {LOG_LEVEL}. Log file: {log_file_path or 'Console only'}")
    return logger

logger = setup_logging()


# --- 3. Download & Extract Helper Functions ---
# (No changes needed in these functions from v1 - they use the URL and target path)
class DownloadError(Exception): pass
class ExtractionError(Exception): pass

def download_file(url: str, dest_file: Path, desc: str = None, max_retries: int = 3) -> bool:
    """Downloads a file from a URL with retries, progress bar, and temporary file handling."""
    if not logger: raise RuntimeError("Logger not initialised. Call setup_logging() first.")
    dest_dir = dest_file.parent
    dest_dir.mkdir(parents=True, exist_ok=True)
    desc = desc or dest_file.name
    temp_dl_path = dest_dir / f"{dest_file.name}.part"

    for attempt in range(1, max_retries + 1):
        try:
            logger.info(f"Attempt {attempt}/{max_retries} to download '{desc}' from {url}")
            response = requests.get(url, stream=True, timeout=60)
            response.raise_for_status()
            total_size = int(response.headers.get('content-length', 0))
            downloaded_size = 0
            with open(temp_dl_path, 'wb') as f, \
                 tqdm(total=total_size, unit='B', unit_scale=True, desc=desc, leave=False) as progress_bar:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        chunk_len = len(chunk)
                        downloaded_size += chunk_len
                        progress_bar.update(chunk_len)

            if total_size != 0 and downloaded_size != total_size:
                raise DownloadError(f"Downloaded size ({downloaded_size}) does not match expected size ({total_size}) for {url}")

            if dest_file.exists():
                logger.warning(f"Destination file {dest_file} exists, overwriting.")
                dest_file.unlink()
            shutil.move(str(temp_dl_path), str(dest_file))
            logger.info(f"Successfully downloaded and saved '{desc}' to {dest_file}")
            return True

        except requests.exceptions.RequestException as e:
            logger.warning(f"Download attempt {attempt} for '{desc}' failed: {e}")
            if temp_dl_path.exists(): temp_dl_path.unlink()
            if attempt == max_retries:
                logger.error(f"Failed to download '{desc}' after {max_retries} attempts.")
                return False
            else:
                wait_time = 3 ** attempt
                logger.info(f"Retrying download in {wait_time} seconds...")
                time.sleep(wait_time)
        except DownloadError as e:
             logger.warning(f"Download attempt {attempt} for '{desc}' failed: {e}")
             if temp_dl_path.exists(): temp_dl_path.unlink()
             if attempt == max_retries:
                 logger.error(f"Failed download validation for '{desc}' after {max_retries} attempts.")
                 return False
             wait_time = 3 ** attempt
             logger.info(f"Retrying download in {wait_time} seconds...")
             time.sleep(wait_time)
        except Exception as e:
            logger.error(f"Unexpected error during download of '{desc}' (Attempt {attempt}): {e!r}", exc_info=True)
            if temp_dl_path.exists(): temp_dl_path.unlink()
            if attempt == max_retries:
                return False
            wait_time = 3 ** attempt
            logger.info(f"Retrying download in {wait_time} seconds...")
            time.sleep(wait_time)
    return False

def extract_zipfile(zip_file: Path, extract_dir: Path, desc: str = None) -> bool:
    """Extracts a zip file safely to a specified directory with progress bar."""
    if not logger: raise RuntimeError("Logger not initialised. Call setup_logging() first.")
    if not zip_file.exists():
        logger.error(f"Zip file not found: {zip_file}")
        return False
    desc = desc or f"Extracting {zip_file.name}"
    extract_dir.mkdir(parents=True, exist_ok=True)
    extract_dir_abs = extract_dir.resolve()

    try:
        with zipfile.ZipFile(zip_file, 'r') as z:
            file_list = z.infolist()
            total_files = len(file_list)
            if total_files == 0:
                logger.warning(f"Zip file '{zip_file.name}' is empty.")
                return True
            extracted_count = 0
            logger.info(f"Starting extraction of {total_files} items from '{zip_file.name}' to '{extract_dir}'")
            with tqdm(total=total_files, desc=desc, leave=False) as progress_bar:
                for file_info in file_list:
                    try:
                        target_path_str = os.path.normpath(file_info.filename)
                        target_path = extract_dir_abs / target_path_str
                        if not str(target_path.resolve()).startswith(str(extract_dir_abs)):
                            logger.warning(f"Skipping potentially unsafe path in zip: '{file_info.filename}'")
                            progress_bar.update(1)
                            continue
                        if file_info.is_dir():
                            target_path.mkdir(parents=True, exist_ok=True)
                        else:
                            target_path.parent.mkdir(parents=True, exist_ok=True)
                            with z.open(file_info.filename) as source, open(target_path, 'wb') as target:
                                shutil.copyfileobj(source, target)
                            extracted_count += 1
                    except Exception as e:
                        logger.error(f"Error extracting item '{file_info.filename}' from '{zip_file.name}': {e!r}", exc_info=True)
                    finally:
                         progress_bar.update(1)
            logger.info(f"Finished extraction: {extracted_count}/{total_files} files extracted successfully from '{zip_file.name}'.")
            return True
    except zipfile.BadZipFile:
        logger.error(f"Error: File '{zip_file}' is not a valid zip file or is corrupted.")
        return False
    except Exception as e:
        logger.error(f"Unexpected error during zip extraction of '{zip_file}': {e!r}", exc_info=True)
        return False

def download_data(urls_dict: Dict[str, str], download_dir: Path, force_download: bool = False) -> bool:
    """Downloads multiple files specified in a dictionary (key=filename, value=URL)."""
    # This function now expects keys to be filenames.
    if not logger: raise RuntimeError("Logger not initialised. Call setup_logging() first.")
    logger.info(f"Checking/Downloading {len(urls_dict)} items to {download_dir}")
    download_dir.mkdir(parents=True, exist_ok=True)
    all_success = True
    downloaded_count = 0
    skipped_count = 0
    failed_count = 0
    items_to_process = list(urls_dict.items())

    with tqdm(total=len(items_to_process), desc="Checking/Downloading Files", leave=True) as pbar:
        for filename_key, url in items_to_process: # Use filename_key from dict
            dest_path = download_dir / filename_key # Use the key as the filename
            pbar.set_postfix_str(f"Checking: {filename_key}", refresh=True)
            if dest_path.exists() and not force_download:
                skipped_count += 1
            else:
                if dest_path.exists() and force_download:
                    logger.info(f"Force download enabled, removing existing file: {dest_path}")
                    try: dest_path.unlink()
                    except OSError as e:
                         logger.error(f"Could not remove existing file {dest_path}: {e}")
                         all_success = False; failed_count += 1; pbar.update(1); continue
                if download_file(url, dest_path, desc=filename_key): # Use filename_key for description
                    downloaded_count += 1
                else:
                    logger.error(f"Failed download for: {filename_key} from {url}")
                    all_success = False; failed_count += 1
            pbar.update(1)
    logger.info(f"Download check finished: Downloaded: {downloaded_count}, Skipped: {skipped_count}, Failed: {failed_count}")
    if not all_success: logger.warning("One or more downloads failed. Check logs above.")
    return all_success


# --- 4. Transformation Helper Functions ---
# (No changes needed in these functions from v1)

def find_geo_column(df: Union[gpd.GeoDataFrame, pd.DataFrame, pl.DataFrame, pl.LazyFrame, Dict[str, Any]],
                    possible_names: List[str]) -> Optional[str]:
    """Finds a column name in various data structures."""
    if not logger: raise RuntimeError("Logger not initialised. Call setup_logging() first.")
    cols = []; schema_dict = None
    if isinstance(df, (pd.DataFrame, gpd.GeoDataFrame)): cols = df.columns.tolist()
    elif isinstance(df, pl.DataFrame): cols = df.columns
    elif isinstance(df, pl.LazyFrame):
        try: schema_dict = df.schema; cols = list(schema_dict.keys())
        except Exception as e: logger.error(f"Could not get schema from Polars LazyFrame: {e}"); return None
    elif isinstance(df, dict): schema_dict = df; cols = list(schema_dict.keys())
    else: logger.error(f"Unsupported type for find_geo_column: {type(df)}"); return None
    if not cols: logger.warning(f"Cannot find columns in the provided object (type: {type(df)})."); return None
    df_columns_upper = {str(col).upper(): str(col) for col in cols}
    # 1. Exact Match
    for name in possible_names:
        name_upper = name.upper()
        if name_upper in df_columns_upper: return df_columns_upper[name_upper]
    # 2. Prefix Match
    for name in possible_names:
        name_upper = name.upper()
        for col_upper, original_name in df_columns_upper.items():
            if col_upper.startswith(name_upper):
                logger.info(f"Found partial prefix match for '{name}': '{original_name}'")
                return original_name
    logger.warning(f"Could not find column matching any of: {possible_names} in columns: {cols}")
    return None

def clean_geo_code(code_val: Any) -> Optional[str]:
    """Cleans and validates a potential geographic code (e.g., SA1, SA2 code)."""
    if code_val is None or pd.isna(code_val): return None
    try:
        if isinstance(code_val, float):
            if code_val.is_integer(): code_str = str(int(code_val)).strip()
            else: return None
        else: code_str = str(code_val).strip()
        if code_str.endswith('.0'): code_str = code_str[:-2]
        if re.fullmatch(r'\d+', code_str) or code_str.upper() == 'AUS': return code_str
        else: return None
    except Exception: return None

def safe_float(value: Any) -> Optional[float]:
    """Safely converts a value to a float."""
    if value is None or pd.isna(value): return None
    try:
        if isinstance(value, str):
            cleaned = value.strip()
            cleaned = re.sub(r'[$,]', '', cleaned)
            if cleaned.startswith('(') and cleaned.endswith(')'): cleaned = '-' + cleaned[1:-1]
            if not cleaned: return None
            return float(cleaned)
        else: return float(value)
    except (ValueError, TypeError): return None

def geometry_to_wkt(geometry: Any) -> Optional[str]:
    """Converts a Shapely geometry to WKT string, attempting to repair invalid geometries."""
    if geometry is None or getattr(geometry, 'is_empty', True): return None
    try:
        if not hasattr(geometry, 'is_valid') or not hasattr(geometry, 'wkt'):
             logger.warning(f"Input is not a recognised geometry object: type={type(geometry)}")
             return None
        if geometry.is_valid: return geometry.wkt
        else:
            # logger.warning(f"Invalid geometry detected: type={geometry.geom_type}. Attempting repair.")
            geometry_fixed = make_valid(geometry)
            if geometry_fixed and not geometry_fixed.is_empty and geometry_fixed.is_valid:
                # logger.debug(f"Geometry repaired successfully.")
                return geometry_fixed.wkt
            else:
                original_wkt_snippet = str(geometry.wkt)[:100] if hasattr(geometry, 'wkt') else 'N/A'
                logger.error(f"Failed to repair geometry or repaired geometry is invalid/empty. Original WKT snippet: {original_wkt_snippet}...")
                return None
    except Exception as e:
        geom_type = type(geometry).__name__; wkt_snippet = 'N/A'
        try:
            if hasattr(geometry, 'wkt'): wkt_snippet = str(geometry.wkt)[:100]
        except Exception: pass
        logger.error(f"Error converting geometry to WKT: {e!r}. Details: Type={geom_type}, WKT Snippet={wkt_snippet}...", exc_info=True)
        return None

def clean_polars_geo_code(series_expr: pl.Expr) -> pl.Expr:
    """Cleans potential geographic codes within a Polars expression."""
    str_expr = series_expr.cast(pl.Utf8, strict=False)
    no_decimal_expr = pl.when(str_expr.str.ends_with(".0")).then(str_expr.str.slice(0, -2)).otherwise(str_expr)
    valid_code_expr = pl.when(no_decimal_expr.str.contains(r"^\d+$") | no_decimal_expr.str.to_uppercase().eq("AUS")).then(no_decimal_expr).otherwise(pl.lit(None, dtype=pl.Utf8))
    return valid_code_expr.alias(series_expr.meta.output_name())

def safe_polars_int(series_expr: pl.Expr) -> pl.Expr:
    """Safely converts a Polars expression (column) to Int64."""
    str_expr = series_expr.cast(pl.Utf8, strict=False)
    cleaned_expr = str_expr.str.replace_all(r"[$,]", "").str.replace(r"^\(\s*(.*)\s*\)$", "-$1")
    int_expr = cleaned_expr.cast(pl.Int64, strict=False)
    return int_expr.alias(series_expr.meta.output_name())

# --- Final print statement for the cell ---
print("Configuration and utility functions defined.")
print("--> Proceed to Cell 5.")

# ================================================
# Cell 4: Configuration & Utility Functions
# ================================================
# @markdown Defines project configuration (paths, URLs, schemas) and reusable helper functions (logging, download, extract, data cleaning).

import logging
import zipfile
import requests
from pathlib import Path
import tempfile
import shutil
import time
import re
import json
from typing import List, Dict, Optional, Union, Any
import os
import sys

# Third-party libraries
import pandas as pd
import geopandas as gpd
import polars as pl
import pyarrow
import pyarrow.parquet as pq
from shapely.geometry import mapping
from shapely.validation import make_valid
from tqdm.notebook import tqdm
import polars.exceptions

# --- 1. Configuration Settings ---
# Uses BASE_DIR defined and verified in Cell 2

# Check if BASE_DIR is defined
if 'BASE_DIR' not in globals() or not isinstance(BASE_DIR, Path):
    raise NameError("BASE_DIR is not defined globally. Please run Cell 2 first.")

# Ensure BASE_DIR exists and is writable
try:
    if not BASE_DIR.exists():
        BASE_DIR.mkdir(parents=True, exist_ok=True)
    test_file = BASE_DIR / ".write_test"
    test_file.write_text("test")
    test_file.unlink()
except Exception as e:
    raise SystemExit(f"Error: BASE_DIR '{BASE_DIR}' is not accessible or writable: {e}")

print(f"Using BASE_DIR: {BASE_DIR}")

# Define derived paths
DATA_DIR = BASE_DIR / 'data'
RAW_DATA_DIR = DATA_DIR / 'raw'
OUTPUT_DIR = BASE_DIR / 'output'
TEMP_DIR = RAW_DATA_DIR / 'temp'
LOG_DIR = BASE_DIR / 'logs'
LOG_FILE = LOG_DIR / 'ahgd_colab_run.log'

# Specific data directories
GEOGRAPHIC_DIR = RAW_DATA_DIR / 'geographic'
CENSUS_DIR = RAW_DATA_DIR / 'census'
TEMP_ZIP_DIR = TEMP_DIR / "zips"
TEMP_EXTRACT_DIR = TEMP_DIR / "extract"

# --- Data source URLs ---
# ***** VERIFY THESE URLS ARE CORRECT ON THE ABS WEBSITE *****
DATA_URLS = {
    'SA1_2021_AUST_GDA2020': 'https://www.abs.gov.au/statistics/standards/australian-statistical-geography-standard-asgs-edition-3/jul2021-jun2026/access-and-downloads/digital-boundary-files/SA1_2021_AUST_GDA2020_SHP.zip',
    'SA2_2021_AUST_GDA2020': 'https://www.abs.gov.au/statistics/standards/australian-statistical-geography-standard-asgs-edition-3/jul2021-jun2026/access-and-downloads/digital-boundary-files/SA2_2021_AUST_GDA2020_SHP.zip',
    'SA3_2021_AUST_GDA2020': 'https://www.abs.gov.au/statistics/standards/australian-statistical-geography-standard-asgs-edition-3/jul2021-jun2026/access-and-downloads/digital-boundary-files/SA3_2021_AUST_GDA2020_SHP.zip',
    'SA4_2021_AUST_GDA2020': 'https://www.abs.gov.au/statistics/standards/australian-statistical-geography-standard-asgs-edition-3/jul2021-jun2026/access-and-downloads/digital-boundary-files/SA4_2021_AUST_GDA2020_SHP.zip',
    'STE_2021_AUST_GDA2020': 'https://www.abs.gov.au/statistics/standards/australian-statistical-geography-standard-asgs-edition-3/jul2021-jun2026/access-and-downloads/digital-boundary-files/STE_2021_AUST_GDA2020_SHP.zip',
    'POA_2021_AUST_GDA2020': 'https://www.abs.gov.au/statistics/standards/australian-statistical-geography-standard-asgs-edition-3/jul2021-jun2026/access-and-downloads/digital-boundary-files/POA_2021_AUST_GDA2020_SHP.zip',
    # ***** REPLACE WITH ACTUAL CENSUS DATAPACK URL(S) NEEDED *****
    'CENSUS_GCP_AUS_2021': 'https://www.abs.gov.au/census/find-census-data/datapacks/download/2021_GCP_ALL_for_AUS.zip',
}

# Geographic levels to process
GEO_LEVELS_SHP_PROCESS = {
    'SA1': 'SA1_2021_AUST_GDA2020',
    'SA2': 'SA2_2021_AUST_GDA2020',
    'SA3': 'SA3_2021_AUST_GDA2020',
    'SA4': 'SA4_2021_AUST_GDA2020',
    'STATE': 'STE_2021_AUST_GDA2020',
    # 'POA': 'POA_2021_AUST_GDA2020'
}

# Geographic levels relevant for Census data processing
GEO_LEVELS_CENSUS_PROCESS = ['SA1', 'SA2']

# Generate required ZIP URLs
REQUIRED_GEO_ZIPS = {}
for level, prefix in GEO_LEVELS_SHP_PROCESS.items():
    if prefix in DATA_URLS:
        zip_filename = f"{prefix}_SHP.zip"
        REQUIRED_GEO_ZIPS[zip_filename] = DATA_URLS[prefix]
    else:
        print(f"Warning: Prefix '{prefix}' for level '{level}' not found in DATA_URLS keys. Cannot download.")

REQUIRED_CENSUS_ZIPS = {}
census_key = 'CENSUS_GCP_AUS_2021'
if census_key in DATA_URLS:
     census_zip_filename = Path(DATA_URLS[census_key]).name
     REQUIRED_CENSUS_ZIPS[census_zip_filename] = DATA_URLS[census_key]
else:
     print(f"Warning: Census DataPack key '{census_key}' not found in DATA_URLS. Census data cannot be downloaded.")

# Census tables to process
CENSUS_TABLE_PATTERNS = {
    "G01": r"2021\s*Census_G01[_\s].*?(" + "|".join(GEO_LEVELS_CENSUS_PROCESS) + r")\.csv$"
}

# Logging Configuration
LOG_LEVEL = 'INFO'
LOG_FORMAT = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'

# Define output schemas
GEO_DIMENSION_SCHEMA = { "geo_key": pl.Utf8, "code": pl.Utf8, "type": pl.Categorical, "name": pl.Utf8, "area_sqkm": pl.Float64, "geometry": pl.Utf8, "year": pl.Int32 }
POPULATION_DIMENSION_SCHEMA = { "pop_key": pl.Utf8, "geo_key": pl.Utf8, "year": pl.Int32, "total_population": pl.Int64, "male_population": pl.Int64, "female_population": pl.Int64, "indigenous_population": pl.Int64 }


# --- 2. Logging Setup ---
logger = None

def setup_logging():
    """Sets up logging configuration to file and console."""
    global logger
    if logger is not None and logger.hasHandlers():
            # Logger already set up
            return logger

    try:
        LOG_DIR.mkdir(parents=True, exist_ok=True)
        if not os.access(LOG_DIR, os.W_OK):
            raise OSError(f"Log directory '{LOG_DIR}' is not writable.")
        log_file_path = LOG_DIR / 'ahgd_colab_run.log'
    except Exception as e:
        print(f"Warning: Could not ensure log directory '{LOG_DIR}'. Logging to console only. Error: {e}", file=sys.stderr)
        log_file_path = None

    log_level_int = getattr(logging, LOG_LEVEL.upper(), logging.INFO)
    root_logger = logging.getLogger()
    root_logger.setLevel(log_level_int) # Set level on root logger

    # Remove and close existing handlers *carefully*
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)
        try:
            # Attempt to close the handler
            handler.close()
        except OSError as e:
            # **Handle the "Transport endpoint not connected" error here**
            if e.errno == 107: # Check if it's the specific error code
                print(f"Warning: Harmless error closing previous log handler (likely due to Drive connection issue): {e}. Continuing setup.", file=sys.stderr)
            else:
                # Log other OS errors encountered during close
                print(f"Warning: Error closing previous log handler: {e}. Continuing setup.", file=sys.stderr)
        except Exception as e:
             # Catch any other unexpected errors during close
             print(f"Warning: Unexpected error closing previous log handler: {e}. Continuing setup.", file=sys.stderr)


    # --- Add New Handlers ---
    formatter = logging.Formatter(LOG_FORMAT)

    # Console Handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(log_level_int) # Use the level set for the application
    console_handler.setFormatter(formatter)
    root_logger.addHandler(console_handler)

    # File Handler (if path is valid)
    if log_file_path:
        try:
            # Use 'a' to append to the log file across runs
            file_handler = logging.FileHandler(log_file_path, mode='a', encoding='utf-8')
            file_handler.setLevel(logging.DEBUG) # Log more details (DEBUG and up) to file
            file_handler.setFormatter(formatter)
            root_logger.addHandler(file_handler)
        except Exception as e:
            print(f"Warning: Failed to create file handler for '{log_file_path}'. Logging will only go to console. Error: {e}", file=sys.stderr)
            log_file_path = None # Ensure log_file_path reflects reality if file handler fails

    # Set Level for noisy libraries
    for lib in ['fiona', 'shapely', 'geopandas', 'urllib3']:
        logging.getLogger(lib).setLevel(logging.WARNING)

    logger = logging.getLogger("ahgd_etl_colab") # Get the application-specific logger
    # Check if handlers were actually added before logging the init message
    if root_logger.hasHandlers():
         logger.info(f"Logging initialised at level {LOG_LEVEL}. Log file: {log_file_path or 'Console only'}")
    else:
         # This case should ideally not happen if console handler works, but as a fallback:
         print("Critical Warning: No logging handlers could be configured.", file=sys.stderr)
         logger = None # Indicate failure

    return logger

# --- Initialise logger ---
logger = setup_logging()


# --- 3. Download & Extract Helper Functions ---
# (No changes from v2)
class DownloadError(Exception): pass
class ExtractionError(Exception): pass

def download_file(url: str, dest_file: Path, desc: str = None, max_retries: int = 3) -> bool:
    """Downloads a file from a URL with retries, progress bar, and temporary file handling."""
    if not logger: raise RuntimeError("Logger not initialised. Call setup_logging() first.")
    dest_dir = dest_file.parent
    dest_dir.mkdir(parents=True, exist_ok=True)
    desc = desc or dest_file.name
    temp_dl_path = dest_dir / f"{dest_file.name}.part"
    for attempt in range(1, max_retries + 1):
        try:
            logger.info(f"Attempt {attempt}/{max_retries} to download '{desc}' from {url}")
            response = requests.get(url, stream=True, timeout=60); response.raise_for_status()
            total_size = int(response.headers.get('content-length', 0)); downloaded_size = 0
            with open(temp_dl_path, 'wb') as f, tqdm(total=total_size, unit='B', unit_scale=True, desc=desc, leave=False) as progress_bar:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk: f.write(chunk); chunk_len = len(chunk); downloaded_size += chunk_len; progress_bar.update(chunk_len)
            if total_size != 0 and downloaded_size != total_size: raise DownloadError(f"Downloaded size ({downloaded_size}) != expected size ({total_size}) for {url}")
            if dest_file.exists(): logger.warning(f"Overwriting existing file: {dest_file}"); dest_file.unlink()
            shutil.move(str(temp_dl_path), str(dest_file)); logger.info(f"Successfully downloaded '{desc}' to {dest_file}"); return True
        except requests.exceptions.RequestException as e: logger.warning(f"DL attempt {attempt} failed: {e}"); err_is_fatal = (attempt == max_retries)
        except DownloadError as e: logger.warning(f"DL attempt {attempt} validation failed: {e}"); err_is_fatal = (attempt == max_retries)
        except Exception as e: logger.error(f"Unexpected DL error (Attempt {attempt}): {e!r}", exc_info=True); err_is_fatal = (attempt == max_retries)
        if temp_dl_path.exists(): temp_dl_path.unlink()
        if err_is_fatal: logger.error(f"Failed DL '{desc}' after {max_retries} attempts."); return False
        else: wait_time = 3 ** attempt; logger.info(f"Retrying DL in {wait_time} seconds..."); time.sleep(wait_time)
    return False # Should not be reached

def extract_zipfile(zip_file: Path, extract_dir: Path, desc: str = None) -> bool:
    """Extracts a zip file safely to a specified directory with progress bar."""
    if not logger: raise RuntimeError("Logger not initialised. Call setup_logging() first.")
    if not zip_file.exists(): logger.error(f"Zip file not found: {zip_file}"); return False
    desc = desc or f"Extracting {zip_file.name}"; extract_dir.mkdir(parents=True, exist_ok=True); extract_dir_abs = extract_dir.resolve()
    try:
        with zipfile.ZipFile(zip_file, 'r') as z:
            file_list = z.infolist(); total_files = len(file_list)
            if total_files == 0: logger.warning(f"Zip file '{zip_file.name}' is empty."); return True
            extracted_count = 0; logger.info(f"Extracting {total_files} items from '{zip_file.name}' to '{extract_dir}'")
            with tqdm(total=total_files, desc=desc, leave=False) as progress_bar:
                for file_info in file_list:
                    try:
                        target_path_str = os.path.normpath(file_info.filename); target_path = extract_dir_abs / target_path_str
                        if not str(target_path.resolve()).startswith(str(extract_dir_abs)): logger.warning(f"Skipping unsafe path: '{file_info.filename}'"); progress_bar.update(1); continue
                        if file_info.is_dir(): target_path.mkdir(parents=True, exist_ok=True)
                        else:
                            target_path.parent.mkdir(parents=True, exist_ok=True)
                            with z.open(file_info.filename) as source, open(target_path, 'wb') as target: shutil.copyfileobj(source, target)
                            extracted_count += 1
                    except Exception as e: logger.error(f"Error extracting '{file_info.filename}': {e!r}", exc_info=False) # Shorter error in loop
                    finally: progress_bar.update(1)
            logger.info(f"Extraction complete: {extracted_count}/{total_files} files extracted."); return True
    except zipfile.BadZipFile: logger.error(f"Bad zip file: '{zip_file}'"); return False
    except Exception as e: logger.error(f"Zip extraction error '{zip_file}': {e!r}", exc_info=True); return False

def download_data(urls_dict: Dict[str, str], download_dir: Path, force_download: bool = False) -> bool:
    """Downloads multiple files (key=filename, value=URL)."""
    if not logger: raise RuntimeError("Logger not initialised. Call setup_logging() first.")
    logger.info(f"Checking/Downloading {len(urls_dict)} items to {download_dir}"); download_dir.mkdir(parents=True, exist_ok=True)
    all_success = True; downloaded_count = 0; skipped_count = 0; failed_count = 0
    items_to_process = list(urls_dict.items())
    with tqdm(total=len(items_to_process), desc="Checking/Downloading Files", leave=True) as pbar:
        for filename_key, url in items_to_process:
            dest_path = download_dir / filename_key; pbar.set_postfix_str(f"Checking: {filename_key}", refresh=True)
            if dest_path.exists() and not force_download: skipped_count += 1
            else:
                if dest_path.exists() and force_download:
                    logger.info(f"Force DL: removing existing {dest_path}");
                    try: dest_path.unlink()
                    except OSError as e: logger.error(f"Could not remove {dest_path}: {e}"); all_success = False; failed_count += 1; pbar.update(1); continue
                if download_file(url, dest_path, desc=filename_key): downloaded_count += 1
                else: logger.error(f"Failed download: {filename_key}"); all_success = False; failed_count += 1
            pbar.update(1)
    logger.info(f"DL check done: DL:{downloaded_count}, Skip:{skipped_count}, Fail:{failed_count}")
    if not all_success: logger.warning("One or more downloads failed."); return False
    return True


# --- 4. Transformation Helper Functions ---
# (No changes from v2)
def find_geo_column(df: Union[gpd.GeoDataFrame, pd.DataFrame, pl.DataFrame, pl.LazyFrame, Dict[str, Any]], possible_names: List[str]) -> Optional[str]:
    """Finds a column name in various data structures."""
    # ... (implementation is unchanged from previous version) ...
    if not logger: raise RuntimeError("Logger not initialised. Call setup_logging() first.")
    cols = []; schema_dict = None
    if isinstance(df, (pd.DataFrame, gpd.GeoDataFrame)): cols = df.columns.tolist()
    elif isinstance(df, pl.DataFrame): cols = df.columns
    elif isinstance(df, pl.LazyFrame):
        try: schema_dict = df.schema; cols = list(schema_dict.keys())
        except Exception as e: logger.error(f"Could not get schema from Polars LazyFrame: {e}"); return None
    elif isinstance(df, dict): schema_dict = df; cols = list(schema_dict.keys())
    else: logger.error(f"Unsupported type for find_geo_column: {type(df)}"); return None
    if not cols: logger.warning(f"Cannot find columns in the provided object (type: {type(df)})."); return None
    df_columns_upper = {str(col).upper(): str(col) for col in cols}
    # 1. Exact Match
    for name in possible_names:
        name_upper = name.upper()
        if name_upper in df_columns_upper: return df_columns_upper[name_upper]
    # 2. Prefix Match
    for name in possible_names:
        name_upper = name.upper()
        for col_upper, original_name in df_columns_upper.items():
            if col_upper.startswith(name_upper):
                logger.info(f"Found partial prefix match for '{name}': '{original_name}'")
                return original_name
    logger.warning(f"Could not find column matching any of: {possible_names} in columns: {cols}")
    return None

def clean_geo_code(code_val: Any) -> Optional[str]:
    """Cleans and validates a potential geographic code (e.g., SA1, SA2 code)."""
    # ... (implementation is unchanged) ...
    if code_val is None or pd.isna(code_val): return None
    try:
        if isinstance(code_val, float):
            if code_val.is_integer(): code_str = str(int(code_val)).strip()
            else: return None
        else: code_str = str(code_val).strip()
        if code_str.endswith('.0'): code_str = code_str[:-2]
        if re.fullmatch(r'\d+', code_str) or code_str.upper() == 'AUS': return code_str
        else: return None
    except Exception: return None

def safe_float(value: Any) -> Optional[float]:
    """Safely converts a value to a float."""
    # ... (implementation is unchanged) ...
    if value is None or pd.isna(value): return None
    try:
        if isinstance(value, str):
            cleaned = value.strip()
            cleaned = re.sub(r'[$,]', '', cleaned)
            if cleaned.startswith('(') and cleaned.endswith(')'): cleaned = '-' + cleaned[1:-1]
            if not cleaned: return None
            return float(cleaned)
        else: return float(value)
    except (ValueError, TypeError): return None

def geometry_to_wkt(geometry: Any) -> Optional[str]:
    """Converts a Shapely geometry to WKT string, attempting to repair invalid geometries."""
    # ... (implementation is unchanged) ...
    if geometry is None or getattr(geometry, 'is_empty', True): return None
    try:
        if not hasattr(geometry, 'is_valid') or not hasattr(geometry, 'wkt'): logger.warning(f"Input not geometry: type={type(geometry)}"); return None
        if geometry.is_valid: return geometry.wkt
        else:
            geometry_fixed = make_valid(geometry)
            if geometry_fixed and not geometry_fixed.is_empty and geometry_fixed.is_valid: return geometry_fixed.wkt
            else: logger.error(f"Failed geometry repair/invalid. WKT snippet: {str(geometry.wkt)[:100] if hasattr(geometry, 'wkt') else 'N/A'}..."); return None
    except Exception as e:
        geom_type = type(geometry).__name__; wkt_snippet = 'N/A'
        try:
            if hasattr(geometry, 'wkt'): wkt_snippet = str(geometry.wkt)[:100]
        except Exception: pass
        logger.error(f"Error converting geometry to WKT: {e!r}. Type={geom_type}, Snippet={wkt_snippet}...", exc_info=True); return None


def clean_polars_geo_code(series_expr: pl.Expr) -> pl.Expr:
    """Cleans potential geographic codes within a Polars expression."""
    # ... (implementation is unchanged) ...
    str_expr = series_expr.cast(pl.Utf8, strict=False)
    no_decimal_expr = pl.when(str_expr.str.ends_with(".0")).then(str_expr.str.slice(0, -2)).otherwise(str_expr)
    valid_code_expr = pl.when(no_decimal_expr.str.contains(r"^\d+$") | no_decimal_expr.str.to_uppercase().eq("AUS")).then(no_decimal_expr).otherwise(pl.lit(None, dtype=pl.Utf8))
    return valid_code_expr.alias(series_expr.meta.output_name())

def safe_polars_int(series_expr: pl.Expr) -> pl.Expr:
    """Safely converts a Polars expression (column) to Int64."""
    # ... (implementation is unchanged) ...
    str_expr = series_expr.cast(pl.Utf8, strict=False)
    cleaned_expr = str_expr.str.replace_all(r"[$,]", "").str.replace(r"^\(\s*(.*)\s*\)$", "-$1")
    int_expr = cleaned_expr.cast(pl.Int64, strict=False)
    return int_expr.alias(series_expr.meta.output_name())

# --- Final print statement for the cell ---
print("Configuration and utility functions defined.")
print("--> Proceed to Cell 5.")

# ================================================
# Cell 5: Core ETL Logic Functions
# ================================================
# @markdown Defines the main processing functions: `process_geography` and `process_census_data`.

# Ensure logger is available from Cell 4
if not logger:
    logger = setup_logging()
    if not logger:
        raise RuntimeError("Failed to initialise logger.")

def process_geography(zip_dir: Path, temp_extract_base: Path, output_dir: Path) -> bool:
    """
    Extracts required ASGS shapefiles, transforms them into a common schema,
    combines them, and saves as geo_dimension.parquet.

    Args:
        zip_dir: Directory containing the downloaded ASGS zip files.
        temp_extract_base: Base directory to extract zip contents into temporarily.
        output_dir: Directory to save the final Parquet file.

    Returns:
        True if processing is successful, False otherwise.
    """
    logger.info("--- Starting Geographic Data Processing ---")
    output_dir.mkdir(parents=True, exist_ok=True) # Ensure output directory exists
    temp_extract_base.mkdir(parents=True, exist_ok=True) # Ensure temp extract base directory exists

    all_geo_data_rows = [] # List to hold data dictionaries for all levels
    overall_success = True # Assume success initially

    # Iterate through the geographic levels defined in Cell 4
    for level, shp_zip_prefix in GEO_LEVELS_SHP_PROCESS.items():
        level_success = False # Track success for this specific level
        logger.info(f"--- Processing Geographic Level: {level} ---")

        shp_zip_filename = f"{shp_zip_prefix}_SHP.zip"
        zip_path = zip_dir / shp_zip_filename

        if not zip_path.exists():
            logger.error(f"Required ZIP file not found for level {level}: {zip_path}. Skipping this level.")
            overall_success = False # Mark overall process as failed if a required zip is missing
            continue # Move to the next level

        # Define extraction target directory specific to this zip file
        # e.g., temp/extract/SA1_2021_AUST_GDA2020_SHP
        extract_target = temp_extract_base / shp_zip_prefix # Extract into a folder named after the prefix

        # --- Extraction ---
        shp_file_path = None
        # Check if already extracted and shapefile exists
        expected_shp_filename = f"{shp_zip_prefix}.shp"
        found_shps = list(extract_target.rglob(f"**/{expected_shp_filename}")) # Search recursively

        if extract_target.exists() and found_shps:
            shp_file_path = found_shps[0] # Assume first match is the correct one
            logger.info(f"Shapefile found in existing extraction directory: {shp_file_path}")
        else:
            # Need to extract
            logger.info(f"Extracting {zip_path.name} to {extract_target}...")
            # Clear target dir first if it exists but doesn't contain the needed SHP
            if extract_target.exists():
                 logger.warning(f"Clearing previous extraction directory: {extract_target}")
                 try:
                     shutil.rmtree(extract_target)
                 except OSError as e:
                     logger.error(f"Failed to remove existing directory {extract_target}: {e}. Skipping level {level}.")
                     overall_success = False
                     continue

            extract_target.mkdir(parents=True, exist_ok=True) # Recreate after potential removal

            if extract_zipfile(zip_path, extract_target, desc=f"Extracting {level}"):
                # Extraction successful, find the shapefile
                found_shps = list(extract_target.rglob(f"**/{expected_shp_filename}"))
                if found_shps:
                    shp_file_path = found_shps[0]
                    logger.info(f"Extraction complete. Found shapefile: {shp_file_path}")
                else:
                    logger.error(f"Extraction completed, but expected shapefile '{expected_shp_filename}' not found in {extract_target}. Skipping level {level}.")
                    overall_success = False
                    continue # Skip this level
            else:
                logger.error(f"Extraction failed for {zip_path.name}. Skipping level {level}.")
                overall_success = False
                continue # Skip this level

        # --- Reading Shapefile ---
        if not shp_file_path:
             logger.error(f"Could not determine shapefile path for level {level}. Skipping.")
             continue

        logger.info(f"Reading shapefile: {shp_file_path}")
        gdf = None
        try:
            read_start_time = time.time()
            # Use GeoPandas to read the shapefile
            gdf = gpd.read_file(shp_file_path)
            read_duration = time.time() - read_start_time
            if gdf is None or gdf.empty:
                 logger.warning(f"Read shapefile for level {level}, but it's empty or failed to load properly.")
                 continue # Skip this level if empty

            logger.info(f"Read {len(gdf)} features from {shp_file_path.name} in {read_duration:.2f} seconds.")

            # --- Transformation ---
            logger.info(f"Transforming {level} data ({len(gdf)} features)...")

            # Define possible column names based on ASGS conventions for 2021
            # The find_geo_column function will try these in order
            code_patterns = [f'{level}_CODE_2021', f'{level}_CODE21', f'{level}_CODE'] # STE_CODE_2021 etc.
            name_patterns = [f'{level}_NAME_2021', f'{level}_NAME21', f'{level}_NAME'] # STE_NAME_2021 etc.
            area_patterns = ['AREA_ALBERS_SQKM', 'Area_SQKM', 'AREASQKM21'] # Prioritise Albers Area

            # Find the actual column names in the GeoDataFrame
            code_col = find_geo_column(gdf, code_patterns)
            name_col = find_geo_column(gdf, name_patterns)
            area_col = find_geo_column(gdf, area_patterns) # Can be None if area column not found/needed

            # --- Critical Check: Code Column ---
            if not code_col:
                logger.error(f"CRITICAL: Could not find unique identifier CODE column for level {level} using patterns {code_patterns}. Columns found: {gdf.columns.tolist()}. Skipping level {level}.")
                overall_success = False
                continue # Cannot proceed without a code column

            if not name_col:
                 logger.warning(f"Could not find NAME column for level {level} using patterns {name_patterns}. Name field will be empty.")
                 # Proceed without name if necessary, but log it

            if not area_col:
                logger.warning(f"Could not find AREA column for level {level} using patterns {area_patterns}. Area field will be null.")
                # Proceed without area if necessary

            # Prepare list for this level's transformed data
            transformed_level_rows = []
            invalid_geom_count = 0
            null_code_count = 0
            processed_count = 0

            # Iterate through GeoDataFrame rows (use itertuples for better performance than iterrows)
            row_iterator = tqdm(gdf.itertuples(index=False), total=len(gdf), desc=f"Transforming {level}", leave=False)
            for row in row_iterator:
                try:
                    # Extract data using getattr (safer for itertuples)
                    code_raw = getattr(row, code_col, None)
                    name_raw = getattr(row, name_col, None) if name_col else None
                    area_raw = getattr(row, area_col, None) if area_col else None
                    geometry_raw = getattr(row, 'geometry', None) # Default geometry column name

                    # Clean and validate data
                    code = clean_geo_code(code_raw)
                    if code is None:
                        null_code_count += 1
                        # logger.warning(f"Row {processed_count}: Null or invalid code encountered: {code_raw}")
                        continue # Skip rows with no valid code

                    name = str(name_raw).strip() if name_raw else f"Unknown {level} {code}" # Provide default name if missing

                    area_sqkm = safe_float(area_raw) # Can be None

                    # --- Geometry Conversion (Using the improved function) ---
                    geom_wkt = geometry_to_wkt(geometry_raw)
                    if geom_wkt is None:
                        invalid_geom_count += 1
                        # logger.warning(f"Row {processed_count} (Code: {code}): Geometry conversion failed or resulted in None.")
                        continue # Skip rows with problematic geometry

                    # Construct the data dictionary matching the schema
                    geo_data = {
                        "geo_key": f"{level}_{code}", # Construct unique key
                        "code": code,
                        "type": level, # Use the loop's level variable
                        "name": name,
                        "area_sqkm": area_sqkm,
                        "geometry": geom_wkt,
                        "year": 2021 # Assuming 2021 data
                    }
                    transformed_level_rows.append(geo_data)
                    level_success = True # Mark level as successful if at least one row is processed

                except Exception as e:
                     logger.error(f"Error processing row {processed_count} for level {level} (Code: {code_raw}): {e!r}", exc_info=True)
                     # Continue processing other rows unless it's a critical error

                finally:
                     processed_count += 1

            # Log warnings for the level if issues occurred
            if invalid_geom_count > 0:
                logger.warning(f"Level {level}: {invalid_geom_count}/{len(gdf)} geometries failed conversion/repair.")
            if null_code_count > 0:
                logger.warning(f"Level {level}: {null_code_count}/{len(gdf)} rows skipped due to null/invalid codes.")

            if transformed_level_rows:
                all_geo_data_rows.extend(transformed_level_rows)
                logger.info(f"Successfully transformed {len(transformed_level_rows)} valid rows for level {level}.")
            else:
                logger.warning(f"No valid rows transformed for level {level}.")
                if level_success is False and null_code_count + invalid_geom_count == len(gdf):
                     # If no rows succeeded and all failed for known reasons, mark level failed but maybe not overall
                     logger.error(f"All rows for level {level} failed processing (invalid code or geometry).")
                     # Keep overall_success potentially True unless zip was missing


        except Exception as e:
            logger.error(f"Failed to read or process shapefile for level {level}: {shp_file_path}. Error: {e!r}", exc_info=True)
            overall_success = False # Mark overall failure if reading/processing fails unexpectedly
            continue # Skip to next level

        finally:
            del gdf # Explicitly delete GeoDataFrame to free memory
            gdf = None
            # Optional: Trigger garbage collection
            # import gc
            # gc.collect()

        # End of loop for one geographic level

    # --- Combine and Save ---
    if not all_geo_data_rows:
        logger.error("No geographic data rows were successfully processed from any level. Cannot create geo_dimension.parquet.")
        return False

    logger.info(f"Combining {len(all_geo_data_rows)} rows from all processed geographic levels.")
    try:
        # Convert list of dicts to Polars DataFrame
        # Specify schema during creation for efficiency and type safety
        final_geo_df = pl.DataFrame(all_geo_data_rows, schema=GEO_DIMENSION_SCHEMA)

        # Ensure correct data types (redundant if schema applied correctly, but safe)
        # final_geo_df = final_geo_df.with_columns([
        #     pl.col(name).cast(dtype, strict=False) for name, dtype in GEO_DIMENSION_SCHEMA.items()
        # ])

        # Select columns in the desired order (matching schema keys)
        final_geo_df = final_geo_df.select(list(GEO_DIMENSION_SCHEMA.keys()))

        # Sort by geo_key for consistency (optional but good practice)
        final_geo_df = final_geo_df.sort("geo_key")

        logger.debug(f"Final Geo DF Schema: {final_geo_df.schema}")
        #logger.debug(f"Final Geo DF Head:\n{final_geo_df.head().to_pandas()}") # Convert to Pandas for head display if needed

        # Define output path
        output_path = output_dir / "geo_dimension.parquet"
        logger.info(f"Writing {final_geo_df.height} rows to {output_path}...")

        write_start_time = time.time()
        # Write to Parquet using Polars
        final_geo_df.write_parquet(output_path, compression='snappy', use_pyarrow=True) # Use snappy compression
        write_duration = time.time() - write_start_time
        try:
             size_mb = output_path.stat().st_size / (1024 * 1024)
             logger.info(f"Successfully wrote geo_dimension.parquet ({size_mb:.2f} MB) in {write_duration:.2f} seconds.")
        except Exception: # Handle potential error getting file size
             logger.info(f"Successfully wrote geo_dimension.parquet in {write_duration:.2f} seconds. (Could not get file size).")

    except Exception as e:
        logger.error(f"Failed to combine data or write geo_dimension.parquet: {e!r}", exc_info=True)
        overall_success = False

    if overall_success:
         logger.info("--- Geographic Data Processing Finished Successfully ---")
    else:
         logger.error("--- Geographic Data Processing Finished with Errors ---")

    return overall_success


def process_census_data(zip_dir: Path, temp_extract_base: Path, output_dir: Path, geo_output_path: Path) -> bool:
    """
    Reads Census G01 CSVs from downloaded Zips, transforms using Polars LazyFrame,
    and saves as population_dimension.parquet.

    Args:
        zip_dir: Directory containing the downloaded Census zip files.
        temp_extract_base: Base directory where zip contents might be extracted (if needed, though scanning is preferred).
        output_dir: Directory to save the final Parquet file.
        geo_output_path: Path to the generated geo_dimension.parquet file (used for joining/validation).

    Returns:
        True if processing is successful, False otherwise.
    """
    logger.info("--- Starting Census Data Processing (G01) ---")
    output_dir.mkdir(parents=True, exist_ok=True)
    # temp_extract_base.mkdir(parents=True, exist_ok=True) # Ensure extract dir exists if needed

    # --- Check if geo_dimension exists ---
    if not geo_output_path.exists():
         logger.error(f"Required geo_dimension.parquet not found at {geo_output_path}. Cannot proceed with population processing.")
         return False

    # Find the Census zip files defined in Cell 4
    census_zip_files = [zip_dir / Path(key) for key in REQUIRED_CENSUS_ZIPS.keys()]
    logger.info(f"Looking for Census zip files: {[p.name for p in census_zip_files]}")

    # Dictionary to hold list of LazyFrames for each table pattern (e.g., "G01")
    lazyframes_by_table: Dict[str, List[pl.LazyFrame]] = {key: [] for key in CENSUS_TABLE_PATTERNS}
    processed_zip_count = 0
    any_zip_found = False
    read_errors = 0

    # Scan inside each required zip file
    for zip_path in tqdm(census_zip_files, desc="Scanning Census Zips", leave=False):
        if not zip_path.exists():
            logger.warning(f"Required Census zip file not found: {zip_path}. Skipping.")
            # Depending on setup, missing one zip might be critical
            # overall_success = False # Decide if this should cause overall failure
            continue

        any_zip_found = True
        logger.info(f"Scanning inside zip: {zip_path.name}")
        try:
            with zipfile.ZipFile(zip_path, 'r') as z:
                csv_files_in_zip = [f.filename for f in z.infolist() if not f.is_dir() and f.filename.lower().endswith('.csv')]
                if not csv_files_in_zip:
                    logger.warning(f"No CSV files found inside {zip_path.name}.")
                    continue

                processed_zip_count += 1
                # Match CSV files against defined patterns (G01, etc.)
                for file_info in tqdm(z.infolist(), desc=f"Scanning {zip_path.name}", leave=False):
                     if file_info.is_dir() or not file_info.filename.lower().endswith('.csv'):
                         continue

                     filename_in_zip = file_info.filename
                     matched_table_code = None

                     # Find which pattern (e.g., "G01") this CSV matches
                     for table_code, pattern in CENSUS_TABLE_PATTERNS.items():
                         try:
                              # Use re.search for flexibility, case-insensitive matching
                              if re.search(pattern, filename_in_zip, re.IGNORECASE):
                                   matched_table_code = table_code
                                   break # Found a match, stop checking patterns
                         except re.error as e:
                              logger.error(f"Invalid regex pattern for table '{table_code}': {pattern}. Error: {e}")
                              continue # Skip this pattern if invalid


                     if not matched_table_code:
                         # logger.debug(f"CSV '{filename_in_zip}' does not match any required patterns. Skipping.")
                         continue

                     # Identify the geographic level from the filename (e.g., SA1, SA2)
                     geo_level_match = None
                     for level in GEO_LEVELS_CENSUS_PROCESS:
                          # Look for _SA1.csv or similar structure, case-insensitive
                          if re.search(rf"[_.\s]{level}\.csv$", filename_in_zip, re.IGNORECASE):
                               geo_level_match = level
                               break

                     if not geo_level_match:
                         logger.warning(f"Could not determine geo level (e.g., SA1, SA2) for matched file '{filename_in_zip}' in {zip_path.name}. Skipping.")
                         continue

                     logger.debug(f"Identified Census CSV: Table={matched_table_code}, Level={geo_level_match}, File='{filename_in_zip}'")

                     # --- Scan CSV using Polars LazyFrame ---
                     try:
                         # Read file content directly from zip into memory buffer
                         with z.open(filename_in_zip) as csv_file:
                              csv_content = csv_file.read()
                              if not csv_content:
                                   logger.warning(f"CSV file '{filename_in_zip}' in {zip_path.name} is empty. Skipping.")
                                   continue

                         # Scan the CSV content lazily
                         lf = pl.scan_csv(
                             csv_content, # Read from bytes
                             has_header=True,
                             separator=',',
                             ignore_errors=True, # Skip rows with parsing errors
                             # Consider specifying dtypes for known columns for efficiency
                             # dtype={'SomeCol': pl.Utf8, ...}
                             null_values=["NA", "N/A", "", "-", "."], # Define null representations
                         )

                         # --- Add geo_level column ---
                         # We know the level from the filename analysis
                         lf = lf.with_columns(pl.lit(geo_level_match).alias("geo_level").cast(pl.Categorical))

                         # Add the LazyFrame to the list for its table type
                         lazyframes_by_table[matched_table_code].append(lf)
                         logger.debug(f"Successfully scanned '{filename_in_zip}' as LazyFrame.")

                     except Exception as e:
                         logger.error(f"Error scanning CSV '{filename_in_zip}' from {zip_path.name}: {e!r}", exc_info=True)
                         read_errors += 1

        except zipfile.BadZipFile:
            logger.error(f"Error: File '{zip_path}' is not a valid zip file or is corrupted.")
            read_errors += 1
            # Decide if this is critical: overall_success = False
        except Exception as e:
            logger.error(f"Unexpected error processing zip file '{zip_path}': {e!r}", exc_info=True)
            read_errors += 1
            # Decide if this is critical: overall_success = False

    # --- Post-Scanning Checks ---
    if not any_zip_found:
        logger.error("No Census ZIP files were found or processed. Cannot create population dimension.")
        return False
    if processed_zip_count == 0 and read_errors > 0:
         logger.error("No Census ZIP files could be processed successfully due to errors.")
         return False
    if read_errors > 0:
        logger.warning(f"{read_errors} errors occurred during CSV scanning. Some data might be missing.")
        # Decide if partial success is acceptable: overall_success might still be True


    # --- Process G01 Table ---
    overall_status = True # Track success for the population part specifically
    pop_success = False # Flag specifically for G01 processing

    if not lazyframes_by_table.get("G01"): # Check if the list exists and is not empty
        logger.warning("No G01 dataframes were scanned. Skipping G01 processing.")
        # If G01 is essential, set overall_status = False here
    else:
        g01_frames = lazyframes_by_table["G01"]
        logger.info(f"Concatenating {len(g01_frames)} scanned G01 LazyFrames.")
        try:
            # Concatenate all G01 LazyFrames vertically
            all_g01_lf = pl.concat(g01_frames, how='vertical')

            # Cache the result after concatenation if memory allows, speeds up subsequent operations
            # all_g01_lf = all_g01_lf.cache()

            # --- Define G01 Transformation (Lazy) ---
            logger.info("Defining G01 transformation (lazy)...")
            g01_schema = all_g01_lf.schema # Get schema after concatenation

            # --- Find Columns using patterns ---
            # Define possible column names based on 2021 Census G01 conventions
            # Use find_geo_column on the schema dictionary
            geo_code_col_options = []
            for level in GEO_LEVELS_CENSUS_PROCESS: # SA1, SA2 etc.
                 geo_code_col_options.extend([f'{level}_CODE_2021', f'{level}_CODE21'])
            # Add the generic 'Region_ID' often used in DataPacks as a fallback
            geo_code_col_options.append('Region_ID')

            primary_code_col = find_geo_column(g01_schema, geo_code_col_options)

            # Use exact G01 column names directly if known and consistent
            total_pop_col = 'Tot_P_P'      # Total Persons
            male_pop_col = 'Tot_P_M'       # Total Males
            female_pop_col = 'Tot_P_F'     # Total Females
            # Indigenous Persons - check exact name in G01 (might vary slightly)
            indig_pop_col = 'Indigenous_P_Tot_P' # Or similar like 'Indig_P_P', 'I_P_P'

            # --- Critical Checks for Required Columns ---
            required_cols = {
                 "Code": primary_code_col,
                 "Total": total_pop_col,
                 "Male": male_pop_col,
                 "Female": female_pop_col,
                 "Indigenous": indig_pop_col
            }
            missing_cols = [name for name, col in required_cols.items() if col not in g01_schema]

            if not primary_code_col: # Specifically check code column again
                 logger.error(f"CRITICAL: Cannot find unique geography CODE column in concatenated G01 data using patterns {geo_code_col_options}. Schema: {g01_schema}. Cannot proceed.")
                 overall_status = False
            elif missing_cols:
                logger.error(f"CRITICAL: Cannot find essential G01 columns: {missing_cols}. Schema: {g01_schema}. Cannot proceed.")
                overall_status = False
            else:
                # All required columns found, proceed with lazy transformation
                logger.info(f"Using G01 columns -> Code:'{primary_code_col}', Total:'{total_pop_col}', Male:'{male_pop_col}', Female:'{female_pop_col}', Indigenous:'{indig_pop_col}'")

                # --- Define Lazy Expressions ---
                # Clean the geographic code and combine with level for geo_key
                geo_key_expr = pl.concat_str([
                     pl.col("geo_level"),
                     pl.lit("_"),
                     clean_polars_geo_code(pl.col(primary_code_col))
                ]).alias("geo_key")

                # Create expressions to safely cast population counts to Int64
                pop_cols_map = {
                     "total_population": total_pop_col,
                     "male_population": male_pop_col,
                     "female_population": female_pop_col,
                     "indigenous_population": indig_pop_col
                }
                cast_exprs = [safe_polars_int(pl.col(source_col)).alias(target_col)
                              for target_col, source_col in pop_cols_map.items()]

                # Select and transform columns
                transformed_pop_lf = (
                    all_g01_lf
                    .with_columns(geo_key_expr) # Create geo_key first
                    .filter(pl.col("geo_key").is_not_null()) # Remove rows where code cleaning failed
                    .select([
                        pl.col("geo_key"),
                        pl.lit(2021).cast(pl.Int32).alias("year"), # Add year column
                        *cast_exprs # Select and cast population columns
                    ])
                    # Generate the pop_key
                    .with_columns(
                        pl.concat_str([
                            pl.lit("POP_"),
                            pl.col("geo_key"),
                            pl.lit("_"),
                            pl.col("year").cast(pl.Utf8)
                        ]).alias("pop_key")
                    )
                    # Select final columns in desired order
                    .select(list(POPULATION_DIMENSION_SCHEMA.keys()))
                    # Drop duplicates based on pop_key (in case of overlapping scans)
                    .unique(subset=["pop_key"], keep='first')
                 )

                # --- Execute Lazy Plan and Write Output ---
                logger.info("Executing G01 transformation and writing population_dimension.parquet...")
                pop_output_path = output_dir / "population_dimension.parquet"
                try:
                    collect_start_time = time.time()
                    # Collect the result into a DataFrame
                    final_pop_df = transformed_pop_lf.collect()
                    collect_duration = time.time() - collect_start_time
                    logger.info(f"Collected {final_pop_df.height} G01 rows in {collect_duration:.2f} seconds.")

                    if final_pop_df.height == 0:
                        logger.warning("Population transformation resulted in an empty DataFrame. Parquet file will not be written.")
                        # Consider if this should be an error: overall_status = False
                    else:
                        # Optional: Join with geo_dimension to check for missing geo_keys
                        try:
                             geo_dim_keys = pl.scan_parquet(geo_output_path).select("geo_key").unique().collect().to_series()
                             missing_geo_keys = final_pop_df.filter(~pl.col("geo_key").is_in(geo_dim_keys))
                             if not missing_geo_keys.is_empty():
                                  logger.warning(f"{missing_geo_keys.height} population rows have geo_keys not found in geo_dimension.parquet. Example: {missing_geo_keys['geo_key'].head(5).to_list()}")
                        except Exception as e:
                             logger.warning(f"Could not validate geo_keys against geo_dimension.parquet: {e!r}")


                        # Sort final DataFrame (optional)
                        final_pop_df = final_pop_df.sort("pop_key")

                        # Write to Parquet
                        write_start_time = time.time()
                        final_pop_df.write_parquet(pop_output_path, compression='snappy', use_pyarrow=True)
                        write_duration = time.time() - write_start_time
                        try:
                             size_mb = pop_output_path.stat().st_size / (1024 * 1024)
                             logger.info(f"Successfully wrote population_dimension.parquet ({size_mb:.2f} MB) in {write_duration:.2f} seconds.")
                             pop_success = True # Mark G01 processing as successful
                        except Exception:
                             logger.info(f"Successfully wrote population_dimension.parquet in {write_duration:.2f} seconds. (Could not get file size).")
                             pop_success = True


                except Exception as e:
                    logger.error(f"Failed to execute G01 transformation or write population_dimension.parquet: {e!r}", exc_info=True)
                    overall_status = False

        except Exception as e:
            logger.error(f"Failed during G01 concatenation or schema analysis: {e!r}", exc_info=True)
            overall_status = False


    # --- Process other Census tables (e.g., G17, G18) ---
    # Add similar blocks here for other tables defined in CENSUS_TABLE_PATTERNS
    # Example:
    # if lazyframes_by_table.get("G17"):
    #     logger.info("Processing G17...")
    #     # ... concatenation, transformation logic for G17 ...
    # else:
    #     logger.info("No G17 data scanned.")

    # --- Final Status ---
    if pop_success: # Check if G01 (or other essential tables) succeeded
         logger.info("--- Census Data Processing Finished Successfully ---")
    else:
         logger.error("--- Census Data Processing Finished with Errors (Essential tables might be missing/failed) ---")
         overall_status = False # Ensure overall status reflects failure if essential parts failed

    return overall_status

print("Core ETL logic functions defined.")
print("--> Proceed to Cell 6.")

# ==============================================================================
# Cell 6: Pipeline Orchestration Function
# ==============================================================================
# @markdown Defines the `run_pipeline` function that manages the sequence of ETL steps.

def run_pipeline(output_dir_arg: Path, raw_dir_arg: Path, skip_progress_arg: bool, force_continue_arg: bool, cleanup_arg: bool):
    """Runs the defined ETL steps: Download, Process Geo, Process Pop."""
    # logger, OUTPUT_DIR etc. are accessible from Cell 4's scope
    global OUTPUT_DIR, RAW_DATA_DIR, TEMP_DIR, TEMP_ZIP_DIR, TEMP_EXTRACT_DIR # Allow modification if needed
    # Set global paths based on arguments
    OUTPUT_DIR = output_dir_arg; RAW_DATA_DIR = raw_dir_arg
    TEMP_DIR = RAW_DATA_DIR / 'temp'; TEMP_ZIP_DIR = TEMP_DIR / "zips"; TEMP_EXTRACT_DIR = TEMP_DIR / "extract"

    # Ensure all necessary directories exist and are writable
    dirs_to_create = [OUTPUT_DIR, RAW_DATA_DIR, TEMP_DIR, TEMP_ZIP_DIR, TEMP_EXTRACT_DIR, LOG_DIR]
    logger.info("Ensuring required directories exist and are writable...")
    all_dirs_ok = True
    for d in dirs_to_create:
        try:
            d.mkdir(parents=True, exist_ok=True)
            if not os.access(d, os.W_OK): logger.critical(f"CRITICAL: Directory exists but is not writable: {d}"); all_dirs_ok = False
            else: logger.debug(f"Directory OK and writable: {d}")
        except OSError as e: logger.critical(f"CRITICAL: Cannot create/access directory {d}: {e}."); all_dirs_ok = False
    if not all_dirs_ok: logger.error("Failed to create/access required directories. Cannot continue."); return False

    logger.info(f"Using Temp Zip Dir: {TEMP_ZIP_DIR}")
    logger.info(f"Using Temp Extract Dir: {TEMP_EXTRACT_DIR}")
    logger.info(f"Using Final Output Dir: {OUTPUT_DIR}")
    logger.info(f"Using Log Dir: {LOG_DIR}")

    overall_status = True # Assume success initially

    # --- Step 1: Download Data ---
    logger.info("=== Download Phase Started ===")
    urls_to_download = {}; urls_to_download.update(SHAPEFILE_URLS)
    for k, v in DATA_URLS.items():
        if k.startswith('census_gcp_'): urls_to_download[k] = v

    # --- CORRECTED CHECK ---
    # Explicitly capture the return value of download_data
    download_phase_success = download_data(urls_to_download, TEMP_ZIP_DIR, not skip_progress_arg)

    if not download_phase_success:
        logger.error("=== Download Phase Failed ===") # Executes if download_data returns False
        overall_status = False
        if not force_continue_arg:
            logger.critical("Exiting due to download errors (force_continue is False).")
            return False # Stop execution
    else:
        logger.info("=== Download Phase Finished Successfully ===") # Executes if download_data returns True
    # --- END CORRECTION ---

    # --- Step 2: ETL Processing ---
    # Only proceed if downloads were okay OR force_continue is True
    if overall_status or force_continue_arg:
        logger.info("=== ETL Phase Started ===")
        etl_success = True # Track success within the ETL phase

        # Process Geography
        logger.info("--- Starting Geography ETL Step ---")
        if not process_geography(TEMP_ZIP_DIR, OUTPUT_DIR, TEMP_EXTRACT_DIR):
             logger.error("--- Geography ETL Step Failed ---")
             etl_success = False # Mark ETL phase as failed
             if not force_continue_arg: overall_status = False # Mark overall failure only if not forcing
        else:
             logger.info("--- Geography ETL Step Finished Successfully ---")

        # Process Population (only if Geo succeeded OR force_continue is on)
        if etl_success or force_continue_arg:
            if not etl_success: logger.warning("Proceeding with Population ETL despite previous Geo errors (force_continue=True).")
            logger.info("--- Starting Population (G01) ETL Step ---")
            if not process_census_data(TEMP_ZIP_DIR, OUTPUT_DIR):
                logger.error("--- Population (G01) ETL Step Failed ---")
                etl_success = False # Mark ETL phase as failed
                if not force_continue_arg: overall_status = False # Mark overall failure only if not forcing
            else:
                logger.info("--- Population (G01) ETL Step Finished Successfully ---")
        # If Geo failed AND we are not forcing continue, this part is skipped anyway
        elif not force_continue_arg:
             logger.warning("Skipping Population ETL due to previous Geography ETL failure (and force_continue is False).")

        if etl_success: logger.info("=== ETL Phase Finished Successfully ===")
        else: logger.error("=== ETL Phase Finished with Errors ===")
        # overall_status is already False if any critical step failed without force_continue
    else:
        # This case happens if downloads failed and force_continue was False
        logger.warning("Skipping ETL Phase due to Download Phase failure.")


    # --- Step 3: Cleanup (Optional) ---
    if cleanup_arg:
        logger.info("=== Cleanup Phase Started ===")
        try:
            if TEMP_EXTRACT_DIR.exists(): logger.info(f"Removing temp extract dir: {TEMP_EXTRACT_DIR}"); shutil.rmtree(TEMP_EXTRACT_DIR)
            else: logger.info("Temp extract dir not found, skipping removal.")
            logger.info("Skipping removal of zip dir (TEMP_ZIP_DIR).") # Keep zips
            logger.info("=== Cleanup Phase Finished ===")
        except Exception as e: logger.error(f"Error during cleanup: {e}", exc_info=True)

    # Return the overall status
    return overall_status

print("Pipeline orchestration function `run_pipeline` defined.")
print("--> Proceed to Cell 7 to execute the pipeline.")

# ==============================================================================
# Cell 7: Execute the ETL Pipeline
# ==============================================================================
# @markdown This cell runs the entire ETL process using the functions defined above.
# @markdown Monitor the output below for progress and error messages.
# @markdown Check the log file (`logs/ahgd_colab_run.log` in your project directory)
# @markdown for detailed logs.

def main_colab_execution():
    """Sets up and runs the ETL pipeline with predefined settings for Colab."""
    # logger, BASE_DIR, OUTPUT_DIR, RAW_DATA_DIR should be accessible from Cell 4

    # Safety check for BASE_DIR
    if 'BASE_DIR' not in globals() or not isinstance(BASE_DIR, Path):
        print("ERROR: BASE_DIR not defined. Please run Cell 2 first.")
        return

    # --- Define Simulation Arguments ---
    class ArgsSim:
        output_dir = str(OUTPUT_DIR)  # Use globally defined path
        raw_dir = str(RAW_DATA_DIR)    # Use globally defined path
        skip_progress = False # Show progress bars
        force_continue = False # Stop if downloads fail (safer)
        cleanup = False # Keep temp extract dir for inspection, keep zips
    args = ArgsSim()
    logger.info(f"Starting ETL with Arguments: {vars(args)}")

    # --- Start Pipeline Execution ---
    print("\nStarting AHGD ETL Pipeline Run...")
    logger.info("================= Pipeline Start =================")
    start_time = time.time()

    # Call the main pipeline function
    success = run_pipeline(
        output_dir_arg=Path(args.output_dir),
        raw_dir_arg=Path(args.raw_dir),
        skip_progress_arg=args.skip_progress,
        force_continue_arg=args.force_continue,
        cleanup_arg=args.cleanup
    )

    end_time = time.time()
    duration = end_time - start_time
    logger.info(f"================= Pipeline End (Duration: {duration:.2f}s) =================")

    # --- Report Final Status ---
    print("\n" + "="*70)
    if success:
        logger.info(f"Pipeline completed successfully in {duration:.2f} seconds.")
        print(f" Pipeline finished successfully in {duration:.2f} seconds.")
        print(f"   Output Directory: {args.output_dir}")
        print(f"   Log File: {LOG_FILE}")

        # Check for output files and report status/size
        try:
            geo_file = Path(args.output_dir) / "geo_dimension.parquet"
            pop_file = Path(args.output_dir) / "population_dimension.parquet"
            print("\nOutput File Status:")
            if geo_file.exists(): print(f"  - geo_dimension.parquet: Exists (Size: {geo_file.stat().st_size / (1024*1024):.2f} MB)")
            else: print(f"  - geo_dimension.parquet:  MISSING")
            if pop_file.exists(): print(f"  - population_dimension.parquet: Exists (Size: {pop_file.stat().st_size / (1024*1024):.2f} MB)")
            else: print(f"  - population_dimension.parquet:  MISSING")

            print(f"\nFiles/Folders in output directory ({args.output_dir}):")
            output_contents = os.listdir(args.output_dir)
            if output_contents: [print(f"  - {item}") for item in output_contents]
            else: print("  (Directory is empty)")
        except Exception as e: print(f"\nWarning: Could not check/list output files: {e}")
    else:
        logger.error(f"Pipeline completed with errors in {duration:.2f} seconds.")
        print(f" Pipeline finished with errors in {duration:.2f} seconds.")
        print(f"   Please check the log file for details: {LOG_FILE}")
        print(f"   Also review the console output above for critical errors.")
    print("="*70 + "\n")

# --- Execute the main function ---
main_colab_execution()
print("\nColab execution function finished.")

# ==============================================================================
# Cell 8: Next Steps & Expansion Ideas (Optional Markdown)
# ==============================================================================
# @markdown ## Next Steps & Expansion
# @markdown
# @markdown *   **Analyse the Output:** Use tools like Polars, DuckDB, or Pandas directly in another notebook to query and analyze the generated `geo_dimension.parquet` and `population_dimension.parquet` files located in your Google Drive output folder.
# @markdown *   **Process More Census Tables:**
# @markdown     *   Modify `CENSUS_TABLE_PATTERNS` in Cell 4 to include patterns for G17, G18, G19, etc.
# @markdown     *   Add new `process_gXX_data` functions in Cell 5, similar to `process_census_data` but tailored to the specific columns and transformations needed for each table.
# @markdown     *   Update the `run_pipeline` function in Cell 6 to call these new processing functions.
# @markdown *   **Add Correspondence:** Implement logic to process the correspondence files (downloaded but currently unused) to create the `bridge_geo_correspondence` table.
# @markdown *   **Refactor (Advanced):** For much larger projects, consider moving utility functions and processing logic into separate `.py` files within your Drive project folder and importing them into a cleaner execution notebook.
# @markdown *   **Memory Monitoring:** Keep an eye on Colab's RAM usage (top right corner) during the `process_geography` step, especially if processing national SA1 data. If it crashes, you might need a high-RAM runtime or explore more advanced memory optimization techniques for GeoPandas.