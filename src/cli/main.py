#!/usr/bin/env python3
"""
AHGD ETL Pipeline - Unified CLI

Command-line interface for the Australian Healthcare Geographic Database ETL Pipeline.
Combines all pipeline functionality: extraction, transformation, validation, and loading.

This unified CLI directly uses the src/ implementation without path manipulation.
"""

import sys
from pathlib import Path
from datetime import datetime
from typing import Optional, List, Dict, Any
import click
from concurrent.futures import ThreadPoolExecutor, as_completed
import time

# Direct imports from modules (absolute imports for entry point compatibility)
from ..utils.logging import get_logger, monitor_performance
from ..utils.config import get_config_manager, is_development
from ..extractors.cli import main as extractor_main
from ..pipelines.cli import main as pipeline_main, run_pipeline
from ..validators.cli import main as validator_main
from ..loaders.cli import main as loader_main


@click.group()
@click.version_option(version="1.0.0")
@click.option('--force-real-data', is_flag=True, 
              help='Force use of real data sources (prevent mock data fallbacks)')
@click.option('--parallel', is_flag=True,
              help='Enable parallel execution where supported')
@click.option('--max-workers', type=int, default=4,
              help='Maximum number of parallel workers (default: 4)')
@click.option('--verbose', '-v', is_flag=True,
              help='Enable verbose logging')
@click.option('--quiet', '-q', is_flag=True, 
              help='Suppress non-essential output')
@click.option('--config', '-c', type=str,
              help='Path to configuration file')
@click.pass_context
def cli(ctx, force_real_data, parallel, max_workers, verbose, quiet, config):
    """AHGD ETL Pipeline - Australian Healthcare Geographic Database ETL.
    
    Unified CLI for data extraction, transformation, validation, and loading.
    Supports parallel execution and prevents mock data fallbacks when requested.
    """
    # Ensure ctx.obj exists for passing context to subcommands
    ctx.ensure_object(dict)
    
    # Store global options in context
    ctx.obj['force_real_data'] = force_real_data
    ctx.obj['parallel'] = parallel
    ctx.obj['max_workers'] = max_workers
    ctx.obj['verbose'] = verbose
    ctx.obj['quiet'] = quiet
    ctx.obj['config'] = config
    
    # Set up logging
    if verbose:
        import logging
        logging.getLogger().setLevel(logging.DEBUG)
    elif quiet:
        import logging
        logging.getLogger().setLevel(logging.WARNING)


# Full pipeline execution command
# The 'run' command is deprecated in V2. Use Airflow for orchestration.


# Individual stage commands with direct integration to existing CLIs
@cli.command()
@click.option('--source', default='all', 
              help='Data source (all, abs, aihw, bom) or specific source ID')
@click.option('--output-dir', default='data_raw',
              help='Output directory for raw data')
@click.option('--force', is_flag=True,
              help='Force re-download existing files')
@click.option('--format', type=click.Choice(['parquet', 'csv', 'json']),
              default='parquet', help='Output format (default: parquet)')
@click.pass_context
def extract(ctx, source, output_dir, force, format):
    """Extract raw data from Australian government sources."""
    
    logger = get_logger(__name__)
    global_options = ctx.obj
    
    click.echo(f"üì• Extracting data from {source.upper()}")
    click.echo(f"üìÅ Output: {output_dir}")
    
    if global_options['force_real_data']:
        click.echo("üåê Real data mode: Mock fallbacks disabled")
    
    # Build arguments for extractor CLI
    args = ['--output', output_dir, '--format', format]
    
    if source == 'all':
        args.append('--all')
    else:
        args.extend(['--sources', source])
    
    if force:
        args.append('--force')
    if global_options['parallel']:
        args.extend(['--max-workers', str(global_options['max_workers'])])
    if global_options['verbose']:
        args.append('--verbose')
    elif global_options['quiet']:
        args.append('--quiet')
    
    # Execute extractor CLI
    original_argv = sys.argv
    sys.argv = ['ahgd-extract'] + args
    
    try:
        result = extractor_main()
        sys.exit(result)
    finally:
        sys.argv = original_argv


@cli.command()
@click.option('--input-dir', default='data_raw',
              help='Input directory with raw data')
@click.option('--output', default='data_processed/master_health_record.parquet',
              help='Output path for processed data')
@click.option('--integration-level',
              type=click.Choice(['minimal', 'standard', 'comprehensive', 'enhanced']),
              default='standard',
              help='Data integration level (default: standard)')
@click.option('--quality-threshold', type=float, default=85.0,
              help='Minimum quality threshold (default: 85.0)')
@click.pass_context
def transform(ctx, input_dir, output, integration_level, quality_threshold):
    """Transform raw data into standardised format."""
    
    logger = get_logger(__name__)
    global_options = ctx.obj
    
    click.echo(f"üîÑ Transforming data from {input_dir} to {output}")
    click.echo(f"üìä Integration level: {integration_level}")
    click.echo(f"‚úÖ Quality threshold: {quality_threshold}%")
    
    if global_options['parallel']:
        click.echo("‚ö° Parallel processing enabled")
    
    # Build arguments for pipeline CLI
    args = [
        '--input', input_dir,
        '--output', output,
        '--integration-level', integration_level,
        '--quality-threshold', str(quality_threshold)
    ]
    
    if global_options['parallel']:
        args.extend(['--max-workers', str(global_options['max_workers'])])
    if global_options['verbose']:
        args.append('--verbose')
    elif global_options['quiet']:
        args.append('--quiet')
    
    # Execute pipeline CLI
    original_argv = sys.argv
    sys.argv = ['ahgd-transform'] + args
    
    try:
        result = pipeline_main()
        sys.exit(result)
    finally:
        sys.argv = original_argv


@cli.command()
@click.option('--input', default='data_processed/master_health_record.parquet',
              help='Input data file to validate')
@click.option('--rules', default='schemas/',
              help='Path to validation rules directory')
@click.option('--report', default='reports/validation_report.html',
              help='Output path for validation report')
@click.option('--detailed', is_flag=True,
              help='Show detailed validation results')
@click.option('--quality-threshold', type=float, default=85.0,
              help='Minimum quality threshold (default: 85.0)')
@click.option('--fail-on-errors', is_flag=True,
              help='Exit with error code if validation errors found')
@click.pass_context
def validate(ctx, input, rules, report, detailed, quality_threshold, fail_on_errors):
    """Validate processed data quality and consistency."""
    
    logger = get_logger(__name__)
    global_options = ctx.obj
    
    click.echo(f"‚úÖ Validating data: {input}")
    click.echo(f"üìã Using rules from: {rules}")
    click.echo(f"üìÑ Report output: {report}")
    
    # Build arguments for validator CLI
    args = [
        '--input', input,
        '--rules', rules,
        '--report', report,
        '--quality-threshold', str(quality_threshold)
    ]
    
    if detailed:
        args.append('--verbose')
    if fail_on_errors:
        args.append('--fail-on-errors')
    if global_options['parallel']:
        args.extend(['--parallel', '--max-workers', str(global_options['max_workers'])])
    if global_options['verbose']:
        args.append('--verbose')
    elif global_options['quiet']:
        args.append('--quiet')
    
    # Execute validator CLI
    original_argv = sys.argv
    sys.argv = ['ahgd-validate'] + args
    
    try:
        result = validator_main()
        sys.exit(result)
    finally:
        sys.argv = original_argv


@cli.command()
@click.option('--input', default='data_processed/master_health_record.parquet',
              help='Input processed data file')
@click.option('--output-dir', default='output',
              help='Output directory for exported data')
@click.option('--formats', multiple=True, default=['parquet', 'csv', 'json'],
              help='Output formats (can specify multiple)')
@click.option('--compress', is_flag=True, default=True,
              help='Enable compression (default: True)')
@click.option('--optimise-web', is_flag=True,
              help='Optimise for web delivery')
@click.pass_context
def load(ctx, input, output_dir, formats, compress, optimise_web):
    """Load/export processed data to specified formats."""
    
    logger = get_logger(__name__)
    global_options = ctx.obj
    
    click.echo(f"üì§ Loading data from {input} to {output_dir}")
    click.echo(f"üìã Formats: {', '.join(formats)}")
    
    # Build arguments for loader CLI
    args = [
        '--input', input,
        '--output', output_dir,
        '--formats'] + list(formats)
    
    if compress:
        args.append('--compress')
    if optimise_web:
        args.append('--optimise-web')
    if global_options['parallel']:
        args.extend(['--parallel', '--max-workers', str(global_options['max_workers'])])
    if global_options['verbose']:
        args.append('--verbose')
    elif global_options['quiet']:
        args.append('--quiet')
    
    # Execute loader CLI
    original_argv = sys.argv
    sys.argv = ['ahgd-load'] + args
    
    try:
        result = loader_main()
        sys.exit(result)
    finally:
        sys.argv = original_argv


# Pipeline management commands
@cli.command()
@click.option('--pipeline', required=True,
              help='Pipeline name to execute')
@click.option('--config-file', type=str,
              help='Configuration file path')
@click.pass_context
def pipeline(ctx, pipeline, config_file):
    """Execute a specific named pipeline."""
    
    logger = get_logger(__name__)
    global_options = ctx.obj
    
    click.echo(f"üîÑ Executing pipeline: {pipeline}")
    
    # Build arguments for pipeline runner
    args = ['--pipeline', pipeline]
    
    if config_file:
        args.extend(['--config', config_file])
    elif global_options['config']:
        args.extend(['--config', global_options['config']])
    
    # Execute pipeline runner
    original_argv = sys.argv
    sys.argv = ['ahgd-pipeline'] + args
    
    try:
        result = run_pipeline()
        sys.exit(result)
    finally:
        sys.argv = original_argv


# Configuration and status commands
@cli.command()
@click.pass_context
def config(ctx):
    """Display current configuration and validate settings."""
    
    logger = get_logger(__name__)
    global_options = ctx.obj
    
    click.echo("‚öôÔ∏è  AHGD ETL Configuration")
    click.echo("=" * 30)
    
    try:
        config_manager = get_config_manager()
        click.echo("‚úÖ Configuration manager available")
        
        # Display configuration information
        click.echo(f"\nüìÅ Project root: {Path.cwd()}")
        click.echo(f"üìÅ Data directories:")
        click.echo(f"   - Raw data: data_raw/")
        click.echo(f"   - Processed: data_processed/")
        click.echo(f"   - Output: output/")
        click.echo(f"   - Reports: reports/")
        
        click.echo(f"\nüîß Global options:")
        click.echo(f"   - Parallel processing: {'Enabled' if global_options['parallel'] else 'Disabled'}")
        if global_options['parallel']:
            click.echo(f"   - Max workers: {global_options['max_workers']}")
        click.echo(f"   - Force real data: {'Yes' if global_options['force_real_data'] else 'No'}")
        click.echo(f"   - Verbose logging: {'Yes' if global_options['verbose'] else 'No'}")
        click.echo(f"   - Config file: {global_options['config'] or 'Default'}")
        
        click.echo("‚úÖ Configuration validated!")
        
    except Exception as e:
        click.echo(f"‚ùå Configuration error: {e}")
        logger.error(f"Configuration error: {e}")
        sys.exit(1)


@cli.command()
def status():
    """Show pipeline status and data availability."""
    
    logger = get_logger(__name__)
    
    click.echo("üìä AHGD ETL Pipeline Status")
    click.echo("=" * 30)
    
    # Check data directories
    data_dirs = {
        'Raw data': 'data_raw',
        'Processed data': 'data_processed', 
        'Output': 'output',
        'Reports': 'reports',
        'Schemas': 'schemas'
    }
    
    for name, dir_path in data_dirs.items():
        path = Path(dir_path)
        if path.exists():
            files = list(path.rglob('*'))
            file_count = len([f for f in files if f.is_file()])
            total_size = sum(f.stat().st_size for f in files if f.is_file())
            size_mb = total_size / (1024 * 1024)
            
            click.echo(f"‚úÖ {name}: {file_count} files ({size_mb:.1f} MB)")
        else:
            click.echo(f"‚ùå {name}: Not found ({dir_path})")
    
    # Check key files
    key_files = {
        'Master health record': 'data_processed/master_health_record.parquet',
        'Latest validation report': 'reports/validation_report.html',
        'Export summary': 'output/export_summary.json'
    }
    
    click.echo(f"\nüìã Key files:")
    for name, file_path in key_files.items():
        path = Path(file_path)
        if path.exists():
            size_mb = path.stat().st_size / (1024 * 1024)
            modified = datetime.fromtimestamp(path.stat().st_mtime).strftime("%Y-%m-%d %H:%M")
            click.echo(f"‚úÖ {name}: {size_mb:.1f} MB (modified: {modified})")
        else:
            click.echo(f"‚ùå {name}: Not found")
    
    click.echo("\nüìà Status check completed!")


# List available commands
@cli.command()
def list_commands():
    """List all available commands and their descriptions."""
    
    click.echo("üõ†Ô∏è  AHGD ETL Pipeline Commands")
    click.echo("=" * 40)
    
    commands = {
        'extract': 'Extract raw data from Australian government sources',
        'transform': 'Transform raw data into standardised format',
        'validate': 'Validate processed data quality and consistency',
        'load': 'Export processed data to multiple formats',
        'pipeline': 'Execute a specific named pipeline',
        'config': 'Display configuration and validate settings',
        'status': 'Show pipeline status and data availability',
        'list-commands': 'List all available commands (this command)'
    }
    
    for cmd, desc in commands.items():
        click.echo(f"\nüìå {cmd}")
        click.echo(f"   {desc}")
    
    click.echo("\nüí° The 'run' command has been deprecated in V2. Please use Apache Airflow for full pipeline orchestration.")
    click.echo(f"üí° Use 'ahgd-etl <command> --help' for detailed command options")
    click.echo(f"üí° Use '--parallel' flag for faster processing on multi-core systems")
    click.echo(f"üí° Use '--force-real-data' to prevent mock data fallbacks")


# Main entry point
@monitor_performance("cli_execution")
def main():
    """Main entry point for the unified AHGD ETL CLI."""
    try:
        cli()
    except KeyboardInterrupt:
        click.echo("\n‚ö†Ô∏è  Operation cancelled by user")
        sys.exit(130)
    except Exception as e:
        logger = get_logger(__name__)
        logger.error(f"Unexpected CLI error: {str(e)}")
        if is_development():
            import traceback
            traceback.print_exc()
        click.echo(f"‚ùå Unexpected error: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()