# Testing Framework Documentation

## Overview

This testing framework provides comprehensive test coverage for the Australian Health Geography Data Analytics (AHGD) project, enabling safe refactoring and ensuring code quality.

## Test Structure

```
tests/
â”œâ”€â”€ __init__.py                     # Test package initialization
â”œâ”€â”€ conftest.py                     # Shared fixtures and configuration
â”œâ”€â”€ fixtures/
â”‚   â””â”€â”€ sample_data.py             # Sample data generation for tests
â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ test_config.py             # Configuration system tests (âœ… 44 tests)
â”‚   â”œâ”€â”€ test_data_processing.py    # Data processing functionality tests
â”‚   â”œâ”€â”€ test_geographic_mapping.py # Geographic mapping tests
â”‚   â””â”€â”€ test_main_application.py   # Main application tests (âœ… 20 tests)
â””â”€â”€ integration/
    â”œâ”€â”€ test_database_operations.py # Database integration tests
    â””â”€â”€ test_dashboard_components.py # Dashboard integration tests
```

## Test Coverage Status

### âœ… Working Test Suites (64 tests passing)

1. **Configuration System** (`test_config.py`) - **96% coverage**
   - Environment configuration
   - Database configuration
   - Dashboard configuration
   - Data source configuration
   - Processing configuration
   - Logging configuration
   - Configuration validation
   - Environment variable loading

2. **Main Application** (`test_main_application.py`) - **100% passing**
   - Application startup and shutdown
   - Project structure validation
   - Configuration integration
   - Error handling
   - Script execution

### ðŸ”§ Test Suites Under Development

3. **Data Processing** (`test_data_processing.py`)
   - CSV/Excel/Parquet data loading
   - Data validation and cleaning
   - Data transformation pipelines
   - Performance testing

4. **Geographic Mapping** (`test_geographic_mapping.py`)
   - Postcode to SA2 mapping
   - Geographic data processing
   - Spatial analysis functions

5. **Database Operations** (`test_database_operations.py`)
   - Database connectivity
   - Data import/export
   - Query performance
   - Backup and recovery

6. **Dashboard Components** (`test_dashboard_components.py`)
   - Streamlit component testing
   - Visualisation generation
   - Interactive features
   - Data filtering

## Running Tests

### Install Dependencies

```bash
uv sync --group test
```

### Run All Tests

```bash
# Run all tests with coverage
python -m pytest

# Run without coverage requirement
python -m pytest --cov-fail-under=0

# Run specific test markers
python -m pytest -m unit
python -m pytest -m integration
python -m pytest -m "not slow"
```

### Run Specific Test Suites

```bash
# Configuration tests (all passing)
python -m pytest tests/unit/test_config.py -v

# Main application tests (all passing)
python -m pytest tests/unit/test_main_application.py -v

# Working test suites only
python -m pytest tests/unit/test_config.py tests/unit/test_main_application.py -v
```

### Run Tests with Coverage Reports

```bash
# Generate HTML coverage report
python -m pytest --cov-report=html

# View coverage report
open htmlcov/index.html
```

## Test Configuration

### Pytest Configuration (`pyproject.toml`)

```toml
[tool.pytest.ini_options]
minversion = "8.0"
testpaths = ["tests"]
python_files = ["test_*.py", "*_test.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
addopts = [
    "--strict-markers",
    "--strict-config",
    "--cov=src",
    "--cov=scripts", 
    "--cov-report=html:htmlcov",
    "--cov-report=term-missing",
    "--cov-report=xml",
    "--cov-fail-under=40",
    "-ra",
    "--tb=short",
]
markers = [
    "unit: Unit tests",
    "integration: Integration tests", 
    "slow: Slow tests that may take longer to run",
    "database: Tests that require database access",
    "network: Tests that require network access",
]
```

### Coverage Configuration

```toml
[tool.coverage.run]
source = ["src", "scripts"]
omit = [
    "*/tests/*",
    "*/test_*", 
    "*/__pycache__/*",
    "*/migrations/*",
    "*/venv/*",
    "*/.venv/*",
]
```

## Test Fixtures and Utilities

### Shared Fixtures (`conftest.py`)

- `test_data_dir`: Path to test data directory
- `project_root`: Path to project root directory  
- `temp_dir`: Temporary directory for tests
- `temp_db`: Temporary DuckDB database
- `sample_*_data`: Pre-generated sample datasets
- `mock_database_connection`: Mocked database connection
- `sample_config`: Sample configuration for testing

### Sample Data (`fixtures/sample_data.py`)

- Health outcome data
- SEIFA socioeconomic data
- Postcode-SA2 correspondence data
- Geographic boundary data
- Demographic data
- Time series data
- Configuration data

## Test Markers

Use pytest markers to organise and run specific test categories:

- `@pytest.mark.unit` - Unit tests
- `@pytest.mark.integration` - Integration tests  
- `@pytest.mark.slow` - Tests that take longer to run
- `@pytest.mark.database` - Tests requiring database access
- `@pytest.mark.network` - Tests requiring network access

## Best Practices

### Writing Tests

1. **Use descriptive test names** - Test names should clearly describe what is being tested
2. **Follow AAA pattern** - Arrange, Act, Assert
3. **One assertion per test** - Each test should verify one specific behaviour
4. **Use fixtures for setup** - Leverage pytest fixtures for test data and setup
5. **Mock external dependencies** - Use mocks for databases, APIs, file systems
6. **Test edge cases** - Include tests for error conditions and boundary values

### Test Organisation

1. **Unit tests** - Test individual functions and classes in isolation
2. **Integration tests** - Test component interactions and workflows
3. **Performance tests** - Test with larger datasets and measure execution time
4. **End-to-end tests** - Test complete user workflows

### Mocking Strategy

1. **Mock external services** - Database connections, HTTP requests, file I/O
2. **Use dependency injection** - Make dependencies injectable for easier testing
3. **Test both success and failure cases** - Mock both normal and error conditions
4. **Verify mock interactions** - Ensure mocked dependencies are called correctly

## Continuous Integration

The testing framework is designed to work in CI/CD environments:

- Tests run on Python 3.11+
- Parallel test execution supported (`pytest-xdist`)
- Coverage reporting in multiple formats (HTML, XML, terminal)
- Test result caching for faster subsequent runs
- Clear test markers for selective test execution

## Current Achievements

âœ… **64 tests passing** with robust testing infrastructure
âœ… **Configuration system 96% tested** - Rock-solid foundation
âœ… **Main application flow tested** - Startup, shutdown, error handling
âœ… **Sample data generation** - Comprehensive test fixtures
âœ… **CI-ready configuration** - Coverage reporting, parallel execution
âœ… **Test organisation** - Clear structure with unit and integration tests

## Next Steps

1. Fix remaining geographic mapping tests
2. Complete data processing test coverage
3. Implement database integration tests  
4. Add dashboard component tests
5. Increase overall coverage to 40%+ target
6. Add performance benchmarking tests

## Troubleshooting

### Common Issues

1. **Coverage below threshold** - Use `--cov-fail-under=0` to disable requirement
2. **Import errors** - Ensure project paths are correctly added to `sys.path`
3. **Mock object errors** - Verify mock setup matches actual object interfaces
4. **Database connection issues** - Use `temp_db` fixture for isolated testing
5. **File path issues** - Use absolute paths and `temp_dir` fixture

### Debug Commands

```bash
# Run with verbose output
python -m pytest -v -s

# Run single test with debugging
python -m pytest tests/unit/test_config.py::TestEnvironment::test_environment_values -v -s

# Show test collection without running
python -m pytest --collect-only

# Run with coverage debug
python -m pytest --cov-report=term-missing --cov-config=.coveragerc
```

This testing framework provides a solid foundation for safe refactoring and continuous code quality improvement.