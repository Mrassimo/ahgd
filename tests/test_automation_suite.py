"""
Test automation and reporting suite.

This module provides automated test execution, reporting, and quality metrics
for the Australian Health Geography Data Analytics system.
"""

import pytest
import pandas as pd
import numpy as np
import time
import json
import csv
from pathlib import Path
from typing import Dict, List, Any, Optional
import sys
import subprocess
import coverage

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent))


class TestAutomationRunner:
    """Automated test runner with comprehensive reporting"""
    
    def __init__(self, project_root: Path = None):
        """Initialize test automation runner"""
        self.project_root = project_root or Path(__file__).parent.parent
        self.test_results = {}
        self.coverage_data = {}
        self.performance_metrics = {}
        
    def run_test_suite(self, test_categories: List[str] = None) -> Dict[str, Any]:
        """Run comprehensive test suite with specified categories"""
        if test_categories is None:
            test_categories = ['unit', 'integration', 'performance', 'visualization']
        
        results = {
            'start_time': time.time(),
            'test_categories': test_categories,
            'results': {},
            'coverage': {},
            'performance': {},
            'summary': {}
        }
        
        for category in test_categories:
            category_results = self._run_test_category(category)
            results['results'][category] = category_results
        
        results['end_time'] = time.time()
        results['total_duration'] = results['end_time'] - results['start_time']
        results['summary'] = self._generate_summary(results)
        
        return results
    
    def _run_test_category(self, category: str) -> Dict[str, Any]:
        """Run specific test category"""
        category_map = {
            'unit': 'tests/unit/',
            'integration': 'tests/integration/',
            'performance': 'tests/test_performance_comprehensive.py',
            'visualization': 'tests/test_visualization_comprehensive.py',
            'scripts': 'tests/test_scripts_comprehensive.py',
            'data': ['tests/test_data_loaders.py', 'tests/test_data_processors.py']
        }
        
        test_path = category_map.get(category, f'tests/test_{category}*.py')
        
        # Run pytest with coverage
        cmd = [
            'python', '-m', 'pytest',
            test_path if isinstance(test_path, str) else ' '.join(test_path),
            '--cov=src',
            '--cov=scripts',
            '--cov-report=json',
            '--cov-report=term-missing',
            '--json-report',
            f'--json-report-file=test_results_{category}.json',
            '-v'
        ]
        
        start_time = time.time()
        try:
            result = subprocess.run(
                ' '.join(cmd), 
                shell=True, 
                capture_output=True, 
                text=True,
                cwd=self.project_root
            )
            
            duration = time.time() - start_time
            
            return {
                'status': 'passed' if result.returncode == 0 else 'failed',
                'duration': duration,
                'return_code': result.returncode,
                'stdout': result.stdout,
                'stderr': result.stderr,
                'test_count': self._extract_test_count(result.stdout)
            }
            
        except Exception as e:
            return {
                'status': 'error',
                'duration': time.time() - start_time,
                'error': str(e),
                'test_count': 0
            }
    
    def _extract_test_count(self, stdout: str) -> Dict[str, int]:
        """Extract test count information from pytest output"""
        counts = {'passed': 0, 'failed': 0, 'error': 0, 'skipped': 0}
        
        # Parse pytest output for test counts
        for line in stdout.split('\n'):
            if 'passed' in line and 'failed' in line:
                # Extract counts from summary line
                parts = line.split()
                for i, part in enumerate(parts):
                    if part.isdigit():
                        if i + 1 < len(parts):
                            status = parts[i + 1].lower().rstrip(',')
                            if status in counts:
                                counts[status] = int(part)
        
        return counts
    
    def _generate_summary(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """Generate comprehensive test summary"""
        total_tests = 0
        total_passed = 0
        total_failed = 0
        total_errors = 0
        
        for category, result in results['results'].items():
            if 'test_count' in result:
                counts = result['test_count']
                total_tests += sum(counts.values())
                total_passed += counts.get('passed', 0)
                total_failed += counts.get('failed', 0)
                total_errors += counts.get('error', 0)
        
        success_rate = (total_passed / total_tests * 100) if total_tests > 0 else 0
        
        return {
            'total_tests': total_tests,
            'passed': total_passed,
            'failed': total_failed,
            'errors': total_errors,
            'success_rate': round(success_rate, 2),
            'duration': results['total_duration'],
            'categories_run': len(results['test_categories'])
        }
    
    def generate_coverage_report(self) -> Dict[str, Any]:
        """Generate comprehensive coverage report"""
        try:
            # Load coverage data from json report
            coverage_file = self.project_root / 'coverage.json'
            if coverage_file.exists():
                with open(coverage_file, 'r') as f:
                    coverage_data = json.load(f)
                
                return self._process_coverage_data(coverage_data)
            
        except Exception as e:
            return {'error': f'Failed to generate coverage report: {e}'}
        
        return {'error': 'Coverage data not found'}
    
    def _process_coverage_data(self, coverage_data: Dict) -> Dict[str, Any]:
        """Process raw coverage data into summary report"""
        files = coverage_data.get('files', {})
        
        total_statements = 0
        total_covered = 0
        module_coverage = {}
        
        for filepath, file_data in files.items():
            statements = file_data.get('summary', {}).get('num_statements', 0)
            covered = statements - file_data.get('summary', {}).get('missing_lines', 0)
            
            total_statements += statements
            total_covered += covered
            
            # Organize by module
            module = self._get_module_name(filepath)
            if module not in module_coverage:
                module_coverage[module] = {'statements': 0, 'covered': 0, 'files': []}
            
            module_coverage[module]['statements'] += statements
            module_coverage[module]['covered'] += covered
            module_coverage[module]['files'].append({
                'file': filepath,
                'coverage': (covered / statements * 100) if statements > 0 else 0
            })
        
        # Calculate module percentages
        for module in module_coverage:
            module_data = module_coverage[module]
            module_data['coverage_percent'] = (
                module_data['covered'] / module_data['statements'] * 100
                if module_data['statements'] > 0 else 0
            )
        
        overall_coverage = (total_covered / total_statements * 100) if total_statements > 0 else 0
        
        return {
            'overall_coverage': round(overall_coverage, 2),
            'total_statements': total_statements,
            'total_covered': total_covered,
            'module_coverage': module_coverage,
            'files_count': len(files)
        }
    
    def _get_module_name(self, filepath: str) -> str:
        """Extract module name from file path"""
        if 'src/' in filepath:
            return filepath.split('src/')[-1].split('/')[0]
        elif 'scripts/' in filepath:
            return 'scripts'
        else:
            return 'other'
    
    def generate_performance_report(self) -> Dict[str, Any]:
        """Generate performance analysis report"""
        # This would typically collect performance metrics from test runs
        # For now, return a template structure
        
        return {
            'benchmark_results': {
                'data_loading': {'avg_time': 0.5, 'max_time': 1.2, 'min_time': 0.2},
                'correlation_calc': {'avg_time': 0.8, 'max_time': 2.1, 'min_time': 0.3},
                'visualization': {'avg_time': 1.2, 'max_time': 3.5, 'min_time': 0.5}
            },
            'memory_usage': {
                'peak_memory_mb': 245,
                'avg_memory_mb': 180,
                'memory_efficient': True
            },
            'scalability': {
                'max_dataset_size': 100000,
                'linear_scaling': True,
                'performance_target_met': True
            }
        }
    
    def export_report(self, results: Dict[str, Any], format: str = 'json') -> Path:
        """Export test results to file"""
        timestamp = time.strftime('%Y%m%d_%H%M%S')
        
        if format == 'json':
            filename = f'test_report_{timestamp}.json'
            filepath = self.project_root / filename
            
            with open(filepath, 'w') as f:
                json.dump(results, f, indent=2, default=str)
                
        elif format == 'csv':
            filename = f'test_summary_{timestamp}.csv'
            filepath = self.project_root / filename
            
            # Create CSV summary
            summary_data = []
            for category, result in results['results'].items():
                summary_data.append({
                    'category': category,
                    'status': result.get('status', 'unknown'),
                    'duration': result.get('duration', 0),
                    'tests_passed': result.get('test_count', {}).get('passed', 0),
                    'tests_failed': result.get('test_count', {}).get('failed', 0)
                })
            
            df = pd.DataFrame(summary_data)
            df.to_csv(filepath, index=False)
            
        elif format == 'html':
            filename = f'test_report_{timestamp}.html'
            filepath = self.project_root / filename
            
            html_content = self._generate_html_report(results)
            with open(filepath, 'w') as f:
                f.write(html_content)
        
        return filepath
    
    def _generate_html_report(self, results: Dict[str, Any]) -> str:
        """Generate HTML test report"""
        html = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>AHGD Test Report</title>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 20px; }}
                .header {{ background-color: #f0f0f0; padding: 20px; }}
                .summary {{ background-color: #e8f5e8; padding: 15px; margin: 10px 0; }}
                .category {{ margin: 15px 0; padding: 10px; border: 1px solid #ddd; }}
                .passed {{ color: green; }}
                .failed {{ color: red; }}
                .error {{ color: orange; }}
                table {{ border-collapse: collapse; width: 100%; }}
                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
                th {{ background-color: #f2f2f2; }}
            </style>
        </head>
        <body>
            <div class="header">
                <h1>Australian Health Geography Data Analytics - Test Report</h1>
                <p>Generated on: {time.strftime('%Y-%m-%d %H:%M:%S')}</p>
            </div>
            
            <div class="summary">
                <h2>Summary</h2>
                <p><strong>Total Tests:</strong> {results['summary']['total_tests']}</p>
                <p><strong>Success Rate:</strong> {results['summary']['success_rate']}%</p>
                <p><strong>Duration:</strong> {results['summary']['duration']:.2f} seconds</p>
            </div>
            
            <h2>Test Categories</h2>
        """
        
        for category, result in results['results'].items():
            status_class = result.get('status', 'unknown')
            html += f"""
            <div class="category">
                <h3>{category.title()} Tests <span class="{status_class}">({result.get('status', 'unknown')})</span></h3>
                <p>Duration: {result.get('duration', 0):.2f} seconds</p>
                <p>Tests: {result.get('test_count', {})}</p>
            </div>
            """
        
        html += """
        </body>
        </html>
        """
        
        return html


class TestQualityMetrics:
    """Calculate and track test quality metrics"""
    
    def __init__(self):
        """Initialize quality metrics calculator"""
        self.metrics = {}
    
    def calculate_code_coverage_quality(self, coverage_data: Dict) -> Dict[str, Any]:
        """Calculate code coverage quality metrics"""
        overall_coverage = coverage_data.get('overall_coverage', 0)
        
        # Quality thresholds
        quality_levels = {
            'excellent': 90,
            'good': 80,
            'adequate': 70,
            'needs_improvement': 60,
            'poor': 0
        }
        
        quality_level = 'poor'
        for level, threshold in quality_levels.items():
            if overall_coverage >= threshold:
                quality_level = level
                break
        
        # Identify modules needing attention
        modules_needing_attention = []
        module_coverage = coverage_data.get('module_coverage', {})
        
        for module, data in module_coverage.items():
            if data.get('coverage_percent', 0) < 70:
                modules_needing_attention.append({
                    'module': module,
                    'coverage': data.get('coverage_percent', 0),
                    'files': len(data.get('files', []))
                })
        
        return {
            'overall_quality': quality_level,
            'coverage_percentage': overall_coverage,
            'modules_needing_attention': modules_needing_attention,
            'quality_score': min(100, overall_coverage + 10)  # Bonus for having tests
        }
    
    def calculate_test_reliability(self, test_results: Dict) -> Dict[str, Any]:
        """Calculate test reliability metrics"""
        total_tests = 0
        total_passed = 0
        flaky_tests = 0
        
        for category, result in test_results.items():
            if 'test_count' in result:
                counts = result['test_count']
                total_tests += sum(counts.values())
                total_passed += counts.get('passed', 0)
                
                # Estimate flaky tests (simplified)
                if counts.get('error', 0) > counts.get('failed', 0):
                    flaky_tests += counts.get('error', 0) - counts.get('failed', 0)
        
        reliability_score = (total_passed / total_tests * 100) if total_tests > 0 else 0
        flakiness_rate = (flaky_tests / total_tests * 100) if total_tests > 0 else 0
        
        return {
            'reliability_score': round(reliability_score, 2),
            'flakiness_rate': round(flakiness_rate, 2),
            'total_tests': total_tests,
            'stable_tests': total_passed,
            'reliability_grade': self._get_reliability_grade(reliability_score)
        }
    
    def _get_reliability_grade(self, score: float) -> str:
        """Get reliability grade based on score"""
        if score >= 95:
            return 'A+'
        elif score >= 90:
            return 'A'
        elif score >= 85:
            return 'B+'
        elif score >= 80:
            return 'B'
        elif score >= 70:
            return 'C'
        else:
            return 'D'
    
    def calculate_performance_metrics(self, performance_data: Dict) -> Dict[str, Any]:
        """Calculate performance quality metrics"""
        benchmarks = performance_data.get('benchmark_results', {})
        
        performance_scores = {}
        for operation, metrics in benchmarks.items():
            avg_time = metrics.get('avg_time', 0)
            max_time = metrics.get('max_time', 0)
            
            # Performance thresholds (in seconds)
            thresholds = {
                'data_loading': {'excellent': 1.0, 'good': 2.0, 'adequate': 5.0},
                'correlation_calc': {'excellent': 1.5, 'good': 3.0, 'adequate': 10.0},
                'visualization': {'excellent': 2.0, 'good': 5.0, 'adequate': 15.0}
            }
            
            operation_thresholds = thresholds.get(operation, {'excellent': 1.0, 'good': 2.0, 'adequate': 5.0})
            
            if avg_time <= operation_thresholds['excellent']:
                score = 'excellent'
            elif avg_time <= operation_thresholds['good']:
                score = 'good'
            elif avg_time <= operation_thresholds['adequate']:
                score = 'adequate'
            else:
                score = 'needs_improvement'
            
            performance_scores[operation] = {
                'score': score,
                'avg_time': avg_time,
                'max_time': max_time
            }
        
        return {
            'operation_scores': performance_scores,
            'overall_performance': self._calculate_overall_performance(performance_scores),
            'memory_efficiency': performance_data.get('memory_usage', {}).get('memory_efficient', True)
        }
    
    def _calculate_overall_performance(self, scores: Dict) -> str:
        """Calculate overall performance grade"""
        score_values = {'excellent': 4, 'good': 3, 'adequate': 2, 'needs_improvement': 1, 'poor': 0}
        
        if not scores:
            return 'unknown'
        
        total_score = sum(score_values.get(data['score'], 0) for data in scores.values())
        avg_score = total_score / len(scores)
        
        if avg_score >= 3.5:
            return 'excellent'
        elif avg_score >= 2.5:
            return 'good'
        elif avg_score >= 1.5:
            return 'adequate'
        else:
            return 'needs_improvement'


# Main test automation execution
def run_comprehensive_test_automation():
    """Run comprehensive test automation suite"""
    print("üöÄ Starting Comprehensive Test Automation Suite")
    print("=" * 60)
    
    # Initialize automation runner
    runner = TestAutomationRunner()
    quality_metrics = TestQualityMetrics()
    
    # Run test suite
    print("üìã Running test categories...")
    test_categories = ['unit', 'visualization', 'scripts', 'data']
    results = runner.run_test_suite(test_categories)
    
    # Generate coverage report
    print("üìä Generating coverage report...")
    coverage_report = runner.generate_coverage_report()
    
    # Generate performance report
    print("‚ö° Analyzing performance metrics...")
    performance_report = runner.generate_performance_report()
    
    # Calculate quality metrics
    print("üéØ Calculating quality metrics...")
    coverage_quality = quality_metrics.calculate_code_coverage_quality(coverage_report)
    reliability_metrics = quality_metrics.calculate_test_reliability(results['results'])
    performance_metrics = quality_metrics.calculate_performance_metrics(performance_report)
    
    # Combine all results
    comprehensive_report = {
        'test_execution': results,
        'coverage_analysis': coverage_report,
        'coverage_quality': coverage_quality,
        'reliability_metrics': reliability_metrics,
        'performance_analysis': performance_report,
        'performance_metrics': performance_metrics,
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
        'production_readiness': {
            'coverage_target_met': coverage_quality['coverage_percentage'] >= 70,
            'reliability_acceptable': reliability_metrics['reliability_score'] >= 80,
            'performance_acceptable': performance_metrics['overall_performance'] in ['excellent', 'good'],
            'ready_for_production': False  # Will be calculated
        }
    }
    
    # Determine production readiness
    comprehensive_report['production_readiness']['ready_for_production'] = all([
        comprehensive_report['production_readiness']['coverage_target_met'],
        comprehensive_report['production_readiness']['reliability_acceptable'],
        comprehensive_report['production_readiness']['performance_acceptable']
    ])
    
    # Export reports
    print("üìÑ Exporting reports...")
    json_report = runner.export_report(comprehensive_report, 'json')
    html_report = runner.export_report(comprehensive_report, 'html')
    
    # Print summary
    print("\n" + "=" * 60)
    print("üéâ TEST AUTOMATION SUMMARY")
    print("=" * 60)
    print(f"üìä Overall Coverage: {coverage_quality['coverage_percentage']:.1f}%")
    print(f"üéØ Reliability Score: {reliability_metrics['reliability_score']:.1f}%")
    print(f"‚ö° Performance Grade: {performance_metrics['overall_performance'].title()}")
    print(f"üöÄ Production Ready: {'‚úÖ YES' if comprehensive_report['production_readiness']['ready_for_production'] else '‚ùå NO'}")
    print(f"üìã Total Tests: {results['summary']['total_tests']}")
    print(f"‚è±Ô∏è  Total Duration: {results['summary']['duration']:.1f}s")
    print(f"üìÑ JSON Report: {json_report}")
    print(f"üåê HTML Report: {html_report}")
    print("=" * 60)
    
    return comprehensive_report


if __name__ == "__main__":
    # Run automation suite if executed directly
    run_comprehensive_test_automation()